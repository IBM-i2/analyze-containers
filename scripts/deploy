#!/usr/bin/env bash
# i2, i2 Group, the i2 Group logo, and i2group.com are trademarks of N.Harris
# Computer Corporation.
# Â© N.Harris Computer Corporation (2022-2023)
#
# SPDX short identifier: MIT

set -e

if [[ -z "${ANALYZE_CONTAINERS_ROOT_DIR}" ]]; then
  echo "ANALYZE_CONTAINERS_ROOT_DIR variable is not set" >&2
  echo "This project should be run inside a VSCode Dev Container. \
  For more information read, the Getting Started guide at \
  https://i2group.github.io/analyze-containers/content/getting_started.html" >&2
  exit 1
fi

USAGE="""
Usage:
  deploy -c <config_name> [-t {clean}] [-v] [-y]
  deploy -c <config_name> [-t {backup|restore} [-b <backup_name>]] [-v] [-y]
  deploy -c <config_name> [-r <release_number>] [-v] [-y]
  deploy -h

Options:
  -c <config_name>                       Name of the config to use.
  -t {clean}                             Clean the deployment. \
Will permanently remove all containers and data.
  -t {backup}                            Backup the database.
  -t {restore}                           Restore the database.
  -b <backup_name>                       Name of the backup to create or \
restore. If not specified, the default backup is used.
  -r <release_number>                    Creates a 'release' change-set, \
with the specified release number.  
  -v                                     Verbose output.
  -y                                     Answer 'yes' to all prompts.
  -h                                     Display the help.
"""

source "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/common_functions.sh"

# Parse arguments
OPTIONS_FOR="deploy"
parse_arguments "$@"
[[ "${TASK}" == "clean" || "${TASK}" == "backup" || "${TASK}" == "restore" ||
  "${TASK}" == "package" || -z "${TASK}" ]] || print_usage 1

if [[ -n "${CHANGE_SET_NUMBER}" ]]; then
  TASK="release"
fi

check_valid_config_name "${CONFIG_NAME}"

if [[ "${TASK}" == "package" ]]; then
  EXTENSIONS_DEV="false"
else
  EXTENSIONS_DEV="true"
fi

if [[ -z "${YES_FLAG}" ]]; then
  YES_FLAG="false"
fi

# Load common functions
source "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/server_functions.sh"
source "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/client_functions.sh"

# Load common variables
source "${ANALYZE_CONTAINERS_ROOT_DIR}/configs/${CONFIG_NAME}/version.conf"
source "${ANALYZE_CONTAINERS_ROOT_DIR}/configs/${CONFIG_NAME}/utils/variables.conf"
source "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/simulated_external_variables.sh"
source "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/common_variables.sh"
source "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/internal_helper_variables.sh"
warn_root_dir_not_in_path
check_docker_is_running
set_dependencies_tag_if_necessary

RELOAD_GATEWAY_REQUIRED="false"
CHANGE_SET_EXISTS="false"
IS_DESTRUCTIVE_CHANGE="false"

###############################################################################
# Create Helper Functions                                                     #
###############################################################################

#######################################
# Bash Array pretending to be a dict (using Parameter Substitution)
# Note we cannot use Bash Associative Arrays as Mac OSX is stuck on Bash 3.x
# Variable 'config_final_action_code' defines the action to take, values are:
# 0 = default, no files changed
# 1 = live, web ui reload
# 2 = requires warm restart
# 3 = database schema change, requires warm/cold application restart
# 4 = server changes, requires a server restart
# Arguments:
#   None
#######################################
function compare_current_configuration() {
  declare -A check_files_array
  check_files_array=(["fmr-match-rules.xml"]=1
    ["system-match-rules.xml"]=1
    ["geospatial-configuration.json"]=1
    ["highlight-queries-configuration.xml"]=1
    ["type-access-configuration.xml"]=1
    ["user.registry.xml"]=1
    ["mapping-configuration.json"]=2
    ["analyze-settings.properties"]=2
    ["analyze-connect.properties"]=2
    ["connectors-template.json"]=2
    ["extension-references.json"]=2
    ["plugin-references.json"]=2
    ["log4j2.xml"]=2
    ["schema-charting-schemes.xml"]=2
    ["schema-results-configuration.xml"]=2
    ["schema-source-reference-schema.xml"]=2
    ["schema-vq-configuration.xml"]=2
    ["command-access-control.xml"]=2
    ["DiscoSolrConfiguration.properties"]=2
    ["schema.xml"]=3
    ["security-schema.xml"]=3
    ["environment/dsid/dsid.properties"]=3
    ["solr/schema-template-definition.xml"]=3
    ["server.extensions.xml"]=4
    ["server.extensions.dev.xml"]=4
    ["web.xml"]=4
    ["jvm.options"]=4)

  # Add gateway schemas
  for gateway_short_name in "${!GATEWAY_SHORT_NAME_SET[@]}"; do
    check_files_array+=(["${gateway_short_name}-schema.xml"]=1)
    check_files_array+=(["${gateway_short_name}-schema-charting-schemes.xml"]=1)
  done

  # Extract synonyms file name (could be only one) from the schema-template-definition.xml
  local synonyms_file
  synonyms_file="$(extract_synonyms_file_name "${CURRENT_CONFIGURATION_PATH}/solr/schema-template-definition.xml")"
  if [[ -n "${synonyms_file}" ]]; then
    check_files_array+=(["solr/${synonyms_file}"]=3)
  fi

  # array used to store file changes (if any)
  config_final_action_code=0
  files_changed_array=()
  print_info "Checking configuration for '${CONFIG_NAME}' ..."

  if [ ! -d "${CURRENT_CONFIGURATION_PATH}" ]; then
    print_error_and_exit "Current configuration path '${CURRENT_CONFIGURATION_PATH}' is not valid (no configuration folder present)"
  fi
  if [ ! -d "${PREVIOUS_CONFIGURATION_PATH}" ]; then
    print_warn "Previous configuration path '${PREVIOUS_CONFIGURATION_PATH}' is not valid (no configuration folder present)"
    return
  fi

  for filename in "${!check_files_array[@]}"; do
    # get action code
    config_action_code="${check_files_array["${filename}"]}"

    # check if filename exists in previous, if so then calc checksum
    if [ -f "${PREVIOUS_CONFIGURATION_PATH}/${filename}" ]; then
      checksum_previous=$(shasum "${PREVIOUS_CONFIGURATION_PATH}/${filename}" | cut -d ' ' -f 1)
    else
      continue
    fi
    # check if filename exists in current, if so then calc checksum
    if [ -f "${CURRENT_CONFIGURATION_PATH}/${filename}" ]; then
      checksum_current=$(shasum "${CURRENT_CONFIGURATION_PATH}/${filename}" | cut -d ' ' -f 1)
    else
      continue
    fi
    # if checksums different then store filename changed and action
    if [[ "$checksum_previous" != "$checksum_current" ]]; then
      print_info "Previous checksum '${checksum_previous}' and current checksum '${checksum_current}' do not match for filename '${filename}'"
      files_changed_array+=("${filename}")
      # set action if higher severity code
      if [[ "${config_action_code}" -gt "${config_final_action_code}" ]]; then
        config_final_action_code="${config_action_code}"
      fi
    fi
  done

  # count number of files changed (elements) in the array
  if [ "${#files_changed_array[@]}" -eq 0 ]; then
    print_info "No checksum differences found, configuration files are in sync"
  else
    print_info "File changes detected, action code is '${config_final_action_code}'"
  fi
  print_info "Results in array '${files_changed_array[*]}'"
}

function compare_current_web_dir_extensions() {
  local checksum_previous checksum_current
  local changes_detected="false"
  # Any change to web-dir-extensions require the same action code 1
  local config_action_code=1

  print_info "Checking web-dir-extensions for '${CONFIG_NAME}' ..."

  if ! [ -d "${PREVIOUS_CONFIGURATION_PATH}/web-dir-extensions" ]; then
    return
  fi
  if ! [ -d "${CURRENT_CONFIGURATION_PATH}/web-dir-extensions" ]; then
    return
  fi
  checksum_previous=$(get_check_sum_of_dir "${PREVIOUS_CONFIGURATION_PATH}/web-dir-extensions")
  checksum_current=$(get_check_sum_of_dir "${CURRENT_CONFIGURATION_PATH}/web-dir-extensions")

  # if checksums different then store dir changed and action
  if [[ "${checksum_previous}" != "${checksum_current}" ]]; then
    print_info "Previous checksum '${checksum_previous}' and current checksum '${checksum_current}' do not match for directory 'web-dir-extensions'"
    files_changed_array+=("web-dir-extensions")
    changes_detected="true"
    # set action if higher severity code
    if [[ "${config_action_code}" -gt "${config_final_action_code}" ]]; then
      config_final_action_code="${config_action_code}"
    fi
  fi

  # count number of files changed (elements) in the array
  if [ "${changes_detected}" == "false" ]; then
    print_info "No checksum differences found, web-dir-extensions are in sync"
  else
    print_info "web-dir-extensions changes detected, action code is '${config_final_action_code}'"
  fi
}

function compare_current_extensions() {
  local changes_detected="false"
  local extension_references_file="${LOCAL_USER_CONFIG_DIR}/extension-references.json"
  local extension_dependencies_path="${EXTENSIONS_DIR}/extension-dependencies.json"
  local extension_names
  # Any change to extensions require the same action code 2
  local config_action_code=2

  print_info "Checking extensions for '${CONFIG_NAME}' ..."
  readarray -t extension_names < <(jq -r '.extensions[] | .name' <"${extension_references_file}")
  for extension in "${extension_names[@]}"; do
    IFS=' ' read -ra dependencies <<<"$(jq -r --arg name "${extension}" '.[] | select(.name == $name) | .dependencies[]' "${extension_dependencies_path}" | xargs)"
    for dependency in "${dependencies[@]}"; do
      if ! is_string_in_array "${dependency}" extension_names; then
        extension_names+=("${dependencies[@]}")
      fi
    done
  done

  for filename in "${extension_names[@]}"; do
    if [ -f "${PREVIOUS_CONFIGURATION_LIB_PATH}/${filename}.sha512" ]; then
      checksum_previous=$(cat "${PREVIOUS_CONFIGURATION_LIB_PATH}/${filename}.sha512")
    else
      continue
    fi
    if [ -f "${PREVIOUS_EXTENSIONS_DIR}/${filename}.sha512" ]; then
      checksum_current=$(cat "${PREVIOUS_EXTENSIONS_DIR}/${filename}.sha512")
    else
      continue
    fi
    # if checksums different then store filename changed and action
    if [[ "${checksum_previous}" != "${checksum_current}" ]]; then
      print_info "Previous checksum '${checksum_previous}' and current checksum '${checksum_current}' do not match for filename '${filename}.sha512'"
      files_changed_array+=("lib/${filename}.sha512")
      changes_detected="true"
      # set action if higher severity code
      if [[ "${config_action_code}" -gt "${config_final_action_code}" ]]; then
        config_final_action_code="${config_action_code}"
      fi
    fi
  done

  if [[ "${changes_detected}" == "false" ]]; then
    print_info "No checksum differences found, extension jars are in sync"
  else
    print_info "Jar changes detected, action code is '${config_final_action_code}'"
  fi
}

function compare_current_plugins() {
  local changes_detected="false"
  local plugin_references_file="${LOCAL_USER_CONFIG_DIR}/plugin-references.json"
  local plugin_files
  # Any change to plugins require the same action code 2
  local config_action_code=2

  print_info "Checking plugins for '${CONFIG_NAME}' ..."
  readarray -t plugin_files < <(jq -r '.plugins[] | .name' <"${plugin_references_file}")
  for filename in "${plugin_files[@]}"; do
    if [ -f "${PREVIOUS_CONFIGURATION_PLUGINS_PATH}/${filename}.sha512" ]; then
      checksum_previous=$(cat "${PREVIOUS_CONFIGURATION_PLUGINS_PATH}/${filename}.sha512")
    else
      continue
    fi
    if [ -f "${PREVIOUS_PLUGINS_DIR}/${filename}.sha512" ]; then
      checksum_current=$(cat "${PREVIOUS_PLUGINS_DIR}/${filename}.sha512")
    else
      continue
    fi
    # if checksums different then store filename changed and action
    if [[ "${checksum_previous}" != "${checksum_current}" ]]; then
      print_info "Previous checksum '${checksum_previous}' and current checksum '${checksum_current}' do not match for filename '${filename}.sha512'"
      files_changed_array+=("plugins/${filename}.sha512")
      # set action if higher severity code
      if [[ "${config_action_code}" -gt "${config_final_action_code}" ]]; then
        config_final_action_code="${config_action_code}"
      fi
    fi
  done

  # count number of files changed (elements) in the array
  if [[ "${changes_detected}" == "false" ]]; then
    print_info "No checksum differences found, plugin files are in sync"
  else
    print_info "Plugin file changes detected, action code is '${config_final_action_code}'"
  fi
}

function compare_current_prometheus_config() {
  local prometheus_template_file="prometheus.yml"
  PROMETHEUS_ACTION_CODE=0 # set to 1 if update required

  print_info "Checking prometheus config for '${CONFIG_NAME}' ..."

  # check if filename exists in previous, if so then calc checksum
  if [ -f "${PREVIOUS_CONFIGURATION_PATH}/prometheus/${prometheus_template_file}" ]; then
    checksum_previous=$(shasum "${PREVIOUS_CONFIGURATION_PATH}/prometheus/${prometheus_template_file}" | cut -d ' ' -f 1)
  fi
  # check if filename exists in current, if so then calc checksum
  if [ -f "${CURRENT_CONFIGURATION_PATH}/prometheus/${prometheus_template_file}" ]; then
    checksum_current=$(shasum "${CURRENT_CONFIGURATION_PATH}/prometheus/${prometheus_template_file}" | cut -d ' ' -f 1)
  fi

  # if checksums different then store filename changed and action
  if [[ "${checksum_previous}" != "${checksum_current}" ]]; then
    print_info "Previous checksum '${checksum_previous}' and current checksum '${checksum_current}' do not match for filename '${prometheus_template_file}'"
    # set prometheus action to 1
    PROMETHEUS_ACTION_CODE=1
  else
    print_info "Prometheus config in sync"
  fi
}

function compare_current_grafana_dashboards() {
  local dashboards_dir="${LOCAL_USER_CONFIG_DIR}/grafana/dashboards"
  GRAFANA_ACTION_CODE=0 # set to 1 if update required

  print_info "Checking grafana config for '${CONFIG_NAME}' ..."

  if ! diff "${CURRENT_CONFIGURATION_PATH}/grafana/dashboards" "${PREVIOUS_CONFIGURATION_PATH}/grafana/dashboards" >/dev/null 2>&1; then
    print_info "Grafana dashboard updates detected"
    GRAFANA_ACTION_CODE=1
    # All  Grafana Dashboards
    for file in "${dashboards_dir}"/*; do
      [[ ! -d "${dashboards_dir}" ]] && continue
      filename="${file##*/}"
      # check if filename exists in previous, if so then calc checksum
      if [ -f "${PREVIOUS_CONFIGURATION_PATH}/grafana/dashboards/${filename}" ]; then
        checksum_previous=$(shasum "${PREVIOUS_CONFIGURATION_PATH}/grafana/dashboards/${filename}" | cut -d ' ' -f 1)
      else
        continue
      fi
      # check if filename exists in current, if so then calc checksum
      if [ -f "${CURRENT_CONFIGURATION_PATH}/grafana/dashboards/${filename}" ]; then
        checksum_current=$(shasum "${CURRENT_CONFIGURATION_PATH}/grafana/dashboards/${filename}" | cut -d ' ' -f 1)
      else
        continue
      fi
      # if checksums different then store filename changed and action
      if [[ "$checksum_previous" != "$checksum_current" ]]; then
        print_info "Previous checksum '${checksum_previous}' and current checksum '${checksum_current}' do not match for filename '${filename}'"
      fi
    done
  else
    print_info "Grafana dashboards are in sync"
  fi
}

function deploy_zk_cluster() {
  print "Running ZooKeeper container"
  run_zk "${ZK1_CONTAINER_NAME}" "${ZK1_FQDN}" "${ZK1_DATA_VOLUME_NAME}" "${ZK1_DATALOG_VOLUME_NAME}" "${ZK1_LOG_VOLUME_NAME}" 1 "zk1"
}

function deploy_solr_cluster() {
  print "Running Solr container"
  run_solr "${SOLR1_CONTAINER_NAME}" "${SOLR1_FQDN}" "${SOLR1_VOLUME_NAME}" "${HOST_PORT_SOLR}" "solr1" "${SOLR_DEBUG_PORT}"
}

function configure_zk_for_solr_cluster() {
  print "Configuring ZooKeeper cluster for Solr"
  run_solr_client_command solr zk mkroot "/${SOLR_CLUSTER_ID}" -z "${ZK_MEMBERS}"
  if [[ "${SOLR_ZOO_SSL_CONNECTION}" == "true" ]]; then
    run_solr_client_command "/opt/solr/server/scripts/cloud-scripts/zkcli.sh" -zkhost "${ZK_HOST}" -cmd clusterprop -name urlScheme -val https
  fi
  run_solr_client_command bash -c "echo \"\${SECURITY_JSON}\" > /tmp/security.json && solr zk cp /tmp/security.json zk:/security.json -z ${ZK_HOST}"
}

function configure_solr_collections() {
  print "Configuring Solr collections"
  delete_folder_if_exists "${LOCAL_CONFIG_DIR}/solr/generated_config"
  run_i2_analyze_tool "/opt/i2-tools/scripts/generateSolrSchemas.sh"
  delete_folder_if_exists "${LOCAL_USER_CONFIG_DIR}/solr/generated_config"
  cp -Rp "${LOCAL_CONFIG_DIR}/solr/generated_config" "${LOCAL_USER_CONFIG_DIR}/solr/generated_config"
  configure_solr_collection "daod_index"
  configure_solr_collection "main_index"
  configure_solr_collection "chart_index"
  configure_solr_collection "highlight_index"
  configure_solr_collection "match_index1" "match_index"
  configure_solr_collection "match_index2" "match_index"
  configure_solr_collection "vq_index"
  configure_solr_collection "recordshare_index"
  if [[ "${DEV_BUILD}" == "true" ]]; then
    configure_solr_collection "main_index_test" "main_index"
  fi
}

function delete_solr_collections() {
  print "Deleting Solr collections"
  delete_solr_collection "main_index"
  delete_solr_collection "match_index1"
  delete_solr_collection "match_index2"
  delete_solr_collection "chart_index"
  delete_solr_collection "daod_index"
  delete_solr_collection "highlight_index"
  delete_solr_collection "vq_index"
  delete_solr_collection "recordshare_index"
  if [[ "${DEV_BUILD}" == "true" ]]; then
    delete_solr_collection "main_index_test"
  fi

  delete_solr_collection_properties "main_index"
  delete_solr_collection_properties "match_index1"
  delete_solr_collection_properties "match_index2"
  delete_solr_collection_properties "chart_index"
  if [[ "${DEV_BUILD}" == "true" ]]; then
    delete_solr_collection_properties "main_index_test"
  fi
}

function create_solr_collections() {
  print "Creating Solr collections"
  create_solr_collection "main_index"
  create_solr_collection "match_index1"
  create_solr_collection "match_index2"
  create_solr_collection "chart_index"
  create_solr_collection "daod_index"
  create_solr_collection "highlight_index"
  create_solr_collection "vq_index"
  create_solr_collection "recordshare_index"
  if [[ "${DEV_BUILD}" == "true" ]]; then
    create_solr_collection "main_index_test"
  fi
}

function create_database() {
  print_info "Removing existing database container"
  case "${DB_DIALECT}" in
  db2)
    delete_container "${DB2_SERVER_CONTAINER_NAME}"
    docker volume rm -f "${DB2_SERVER_VOLUME_NAME}" "${DB2_SERVER_BACKUP_VOLUME_NAME}"
    create_folder "${BACKUP_DIR}"
    initialize_db2_server
    ;;
  sqlserver)
    delete_container "${SQL_SERVER_CONTAINER_NAME}"
    docker volume rm -f "${SQL_SERVER_VOLUME_NAME}" "${SQL_SERVER_BACKUP_VOLUME_NAME}"
    create_folder "${BACKUP_DIR}"
    initialize_sql_server
    ;;
  postgres)
    delete_container "${POSTGRES_SERVER_CONTAINER_NAME}"
    docker volume rm -f "${POSTGRES_SERVER_VOLUME_NAME}" "${POSTGRES_SERVER_BACKUP_VOLUME_NAME}"
    create_folder "${BACKUP_DIR}"
    initialize_postgres_server
    ;;
  esac
}

function restore_database() {
  case "${DB_DIALECT}" in
  db2)
    restore_db2_server
    ;;
  sqlserver)
    restore_sql_server
    ;;
  postgres)
    restore_postgres_server
    ;;
  esac
}

function restore_db2_server() {
  deploy_secure_db2_server
  restore_istore_database
}

function restore_sql_server() {
  deploy_secure_sql_server
  restore_istore_database
  recreate_sql_server_users
}

function restore_postgres_server() {
  deploy_secure_postgres_server
  # DB needs to be created first with all users
  run_postgres_server_command_as_postgres "/opt/databaseScripts/generated/runDatabaseCreationScripts.sh"
  recreate_postgres_server_users

  restore_istore_database
}

function restore_istore_database() {
  if [[ -z "${BACKUP_NAME}" ]]; then
    print "No backup_name provided, using the 'default' name"
    BACKUP_NAME="default"
  fi

  case "${DB_DIALECT}" in
  db2)
    update_volume "${BACKUP_DIR}" "${DB2_SERVER_BACKUP_VOLUME_NAME}" "${DB_CONTAINER_BACKUP_DIR}"
    run_db_container_with_backup_volume chown -R 1000:1000 "${DB_CONTAINER_BACKUP_DIR}/${BACKUP_NAME}"

    print "Restoring the ${DB_NAME} database"
    sql_query="\
      RESTORE DATABASE ${DB_NAME} USER \"\${DB_USERNAME}\" USING \"\${DB_PASSWORD}\" FROM \"${DB_CONTAINER_BACKUP_DIR}/${BACKUP_NAME}\""
    run_db2_server_command_as_db2inst1 run-sql-query "${sql_query}"
    ;;
  sqlserver)
    update_volume "${BACKUP_DIR}" "${SQL_SERVER_BACKUP_VOLUME_NAME}" "${DB_CONTAINER_BACKUP_DIR}"
    run_db_container_with_backup_volume chown -R mssql:0 "${DB_CONTAINER_BACKUP_DIR}/${BACKUP_NAME}"

    print "Restoring the ${DB_NAME} database"
    sql_query="\
      RESTORE DATABASE ${DB_NAME} FROM DISK = '${DB_CONTAINER_BACKUP_DIR}/${BACKUP_NAME}/${DB_BACKUP_FILE_NAME}';"
    run_sql_server_command_as_sa run-sql-query "${sql_query}"
    ;;
  postgres)
    update_volume "${BACKUP_DIR}" "${POSTGRES_SERVER_BACKUP_VOLUME_NAME}" "${DB_CONTAINER_BACKUP_DIR}"
    run_db_container_with_backup_volume chown -R postgres:0 "${DB_CONTAINER_BACKUP_DIR}/${BACKUP_NAME}"

    print "Restoring the ${DB_NAME} database"
    docker exec "${POSTGRES_SERVER_CONTAINER_NAME}" bash -c "${SQLCMD} ${DB_NAME} < ${DB_CONTAINER_BACKUP_DIR}/${BACKUP_NAME}/${DB_BACKUP_FILE_NAME}"
    ;;
  esac
}

function recreate_sql_server_users() {
  print "Dropping ISTORE users"
  sql_query="\
    USE ISTORE;
      DROP USER dba;
        DROP USER i2analyze;
          DROP USER i2etl;
            DROP USER etl;
              DROP USER dbb;"
  run_sql_server_command_as_sa run-sql-query "${sql_query}"

  print "Creating i2_Public_Role"
  sql_query="\
    USE ISTORE;
      IF DATABASE_PRINCIPAL_ID(N'i2_Public_Role') IS NULL
          CREATE ROLE i2_Public_Role;
              GRANT CONNECT TO i2_Public_Role;
                    GRANT ALTER, SELECT, UPDATE, INSERT, DELETE, EXEC ON SCHEMA::IS_PUBLIC TO i2_Public_Role;"
  run_sql_server_command_as_sa run-sql-query "${sql_query}"

  print "Creating database logins and users"
  create_db_login_and_user "dbb" "db_backupoperator"
  create_db_login_and_user "dba" "DBA_Role"
  create_db_login_and_user "i2analyze" "i2Analyze_Role"
  create_db_login_and_user "i2etl" "i2_ETL_Role"
  create_db_login_and_user "etl" "External_ETL_Role"
  run_sql_server_command_as_sa "/opt/db-scripts/configure_dba_roles_and_permissions.sh"
  run_sql_server_command_as_sa "/opt/db-scripts/add_etl_user_to_sys_admin_role.sh"

  print "Assigning additional database roles to users"
  assign_additional_roles_to_users
}

function recreate_postgres_server_users() {
  print_info "Creating database logins and users"
  run_postgres_server_command_as_postgres "/opt/db-scripts/create_db_roles.sh"
  create_db_login_and_user "dba" "DBA_Role"
  run_postgres_server_command_as_dba "/opt/db-scripts/grant_permissions_to_roles.sh"
  create_db_login_and_user "dbb" "DBB_Role"
  create_db_login_and_user "i2analyze" "i2Analyze_Role"
  create_db_login_and_user "i2etl" "i2_ETL_Role"
  create_db_login_and_user "etl" "External_ETL_Role"

  print "Assigning additional database roles to users"
  assign_additional_roles_to_users
}

function initialize_db2_server() {
  deploy_secure_db2_server
  initialize_istore_database
  configure_istore_database
  docker exec "${DB2_SERVER_CONTAINER_NAME}" bash -c "su -p db2inst1 -c \". ${DB_LOCATION_DIR}/sqllib/db2profile && db2 UPDATE DB CFG FOR ${DB_NAME} USING extbl_location '${DB_LOCATION_DIR};/var/i2a-data'\""
}

function deploy_secure_db2_server() {
  run_db2_server
  wait_for_db2_server_to_be_live "true"
  change_db2_inst1_password
}

function initialize_sql_server() {
  deploy_secure_sql_server
  initialize_istore_database
  configure_istore_database
}

function deploy_secure_sql_server() {
  run_sql_server

  wait_for_sql_server_to_be_live "true"
  change_sa_password
}

function initialize_postgres_server() {
  deploy_secure_postgres_server
  initialize_istore_database
  configure_istore_database
}

function deploy_secure_postgres_server() {
  run_postgres_server

  wait_for_postgres_server_to_be_live "true"
  change_postgres_password
}

function initialize_istore_database_for_db2_server() {
  run_db2_server_command_as_db2inst1 "/opt/databaseScripts/generated/runDatabaseCreationScripts.sh"

  print "Initializing ISTORE database tables"
  run_db2_server_command_as_db2inst1 "/opt/databaseScripts/generated/runStaticScripts.sh"
}

function initialize_istore_database_for_sql_server() {
  run_sql_server_command_as_sa "/opt/databaseScripts/generated/runDatabaseCreationScripts.sh"

  print_info "Creating database roles"
  run_sql_server_command_as_sa "/opt/db-scripts/create_db_roles.sh"

  print_info "Creating database logins and users"
  create_db_login_and_user "dbb" "db_backupoperator"
  create_db_login_and_user "dba" "DBA_Role"
  create_db_login_and_user "i2analyze" "i2Analyze_Role"
  create_db_login_and_user "i2etl" "i2_ETL_Role"
  create_db_login_and_user "etl" "External_ETL_Role"
  run_sql_server_command_as_sa "/opt/db-scripts/configure_dba_roles_and_permissions.sh"
  run_sql_server_command_as_sa "/opt/db-scripts/add_etl_user_to_sys_admin_role.sh"

  print "Initializing ISTORE database tables"
  run_sql_server_command_as_dba "/opt/databaseScripts/generated/runStaticScripts.sh"
}

function initialize_istore_database_for_postgres_server() {
  run_postgres_server_command_as_postgres "/opt/databaseScripts/generated/runDatabaseCreationScripts.sh"

  print_info "Creating database roles"
  run_postgres_server_command_as_postgres "/opt/db-scripts/create_db_roles.sh"
  create_db_login_and_user "dba" "DBA_Role"
  run_postgres_server_command_as_dba "/opt/db-scripts/grant_permissions_to_roles.sh"

  print_info "Creating database logins and users"
  create_db_login_and_user "dbb" "DBB_Role"
  create_db_login_and_user "i2analyze" "i2Analyze_Role"
  create_db_login_and_user "i2etl" "i2_ETL_Role"
  create_db_login_and_user "etl" "External_ETL_Role"

  print "Initializing ISTORE database tables"
  run_postgres_server_command_as_dba "/opt/databaseScripts/generated/runStaticScripts.sh"
}

function initialize_istore_database() {
  print "Initializing ISTORE database"

  print_info "Generating ISTORE scripts"
  run_i2_analyze_tool "/opt/i2-tools/scripts/generateInfoStoreToolScripts.sh"
  run_i2_analyze_tool "/opt/i2-tools/scripts/generateStaticInfoStoreCreationScripts.sh"

  print_info "Running ISTORE static scripts"
  case "${DB_DIALECT}" in
  db2)
    initialize_istore_database_for_db2_server
    ;;
  sqlserver)
    initialize_istore_database_for_sql_server
    ;;
  postgres)
    initialize_istore_database_for_postgres_server
    ;;
  esac
}

function configure_istore_database() {
  print "Configuring ISTORE database"
  run_i2_analyze_tool "/opt/i2-tools/scripts/generateDynamicInfoStoreCreationScripts.sh"
  case "${DB_DIALECT}" in
  db2)
    run_db2_server_command_as_db2inst1 "/opt/databaseScripts/generated/runDynamicScripts.sh"
    ;;
  sqlserver)
    run_sql_server_command_as_dba "/opt/databaseScripts/generated/runDynamicScripts.sh"
    ;;
  postgres)
    run_postgres_server_command_as_dba "/opt/databaseScripts/generated/runDynamicScripts.sh"
    ;;
  esac

  print "Assigning additional database roles to users"
  assign_additional_roles_to_users

  if [[ -f "${LOCAL_CUSTOM_DB_SCRIPTS_DIR}/informationStoreModifications.sql" ]]; then
    print "Run custom modification script"
    case "${DB_DIALECT}" in
    db2)
      run_db2_server_command_as_db2inst1 run-sql-file "/opt/customDatabaseScripts/informationStoreModifications.sql"
      ;;
    sqlserver)
      run_sql_server_command_as_dba run-sql-file "/opt/customDatabaseScripts/informationStoreModifications.sql"
      ;;
    postgres)
      run_postgres_server_command_as_dba run-sql-file "/opt/customDatabaseScripts/informationStoreModifications.sql"
      ;;
    esac
  fi
}

function assign_additional_roles_to_users() {
  add_user_to_role "i2analyze" "i2_Public_Role"
  add_user_to_role "i2etl" "Deletion_By_Rule"
}

function deploy_liberty() {
  print "Building Liberty Configured image"
  build_liberty_configured_image
  print "Running Liberty container"
  run_liberty "${LIBERTY1_CONTAINER_NAME}" "${I2_ANALYZE_FQDN}" "${LIBERTY1_VOLUME_NAME}" "${HOST_PORT_I2ANALYZE_SERVICE}" "${I2_ANALYZE_CERT_FOLDER_NAME}" "${LIBERTY1_DEBUG_PORT}"
}

function restart_connectors_for_config() {
  local connector_references_file="${LOCAL_USER_CONFIG_DIR}/connector-references.json"

  readarray -t all_connector_ids < <(jq -r '.connectors // empty | .[].name' <"${connector_references_file}")

  for connector_name in "${all_connector_ids[@]}"; do
    container_id=$(docker ps -a -q -f name="^${CONNECTOR_PREFIX}${connector_name}$" -f status=exited)
    if [[ -n ${container_id} ]]; then
      print "Restarting connector container"
      docker start "${CONNECTOR_PREFIX}${connector_name}"
      RELOAD_GATEWAY_REQUIRED="true"
    fi
  done
}

function start_connectors_for_config() {
  local connector_references_file="${LOCAL_USER_CONFIG_DIR}/connector-references.json"

  readarray -t all_connector_ids < <(jq -r '.connectors // empty | .[].name' <"${connector_references_file}")

  for connector_name in "${all_connector_ids[@]}"; do
    container_id=$(docker ps -a -q -f name="^${CONNECTOR_PREFIX}${connector_name}$" -f status=running)
    if [[ -z "${container_id}" ]]; then
      print "Starting connector container"
      deploy_connector "${connector_name}"
      RELOAD_GATEWAY_REQUIRED="true"
    fi
  done
  update_connector_url_mappings
}

function recreate_deployment() {
  initialize_deployment
  remove_all_containers_for_the_config "${CONFIG_NAME}"

  deploy_zk_cluster
  deploy_solr_cluster
  update_solr_jars_for_dev
  wait_for_solr_to_be_live "${SOLR1_FQDN}"

  build_connectors
  start_connectors_for_config
  deploy_metrics

  case "${DB_DIALECT}" in
  db2)
    print_error_and_exit "DB2 is not supported. You will need to clean your deployment first."
    ;;
  sqlserver)
    run_sql_server
    wait_for_sql_server_to_be_live
    ;;
  postgres)
    run_postgres_server
    wait_for_postgres_server_to_be_live
    ;;
  esac

  build_extensions
  build_plugins
  deploy_liberty
  liberty_msg=$(check_liberty_status)
  if [[ "${DEPLOYMENT_PATTERN}" == *"store"* ]]; then
    update_match_rules
    download_highlight_queries_xsd
  fi
  update_state_file 4

  echo "${liberty_msg}"
  print "Recreated deployment successfully"
  echo "This application is configured for access on ${FRONT_END_URI}"
}

function create_deployment() {
  local liberty_msg

  initialize_deployment

  if [[ "${STATE}" == 0 ]]; then
    # Cleaning up Docker resources
    remove_all_containers_for_the_config "${CONFIG_NAME}"
    remove_docker_volumes

    # Running Solr and ZooKeeper
    deploy_zk_cluster
    configure_zk_for_solr_cluster
    deploy_solr_cluster
    update_solr_jars_for_dev

    # Configuring Solr and ZooKeeper
    wait_for_solr_to_be_live "${SOLR1_FQDN}"
    configure_solr_collections
    create_solr_collections
    update_state_file 1
  fi

  build_connectors
  start_connectors_for_config

  if [[ "${STATE}" == 0 || "${STATE}" == 1 ]]; then

    deploy_metrics
    # Configuring ISTORE
    if [[ "${DEPLOYMENT_PATTERN}" == *"store"* ]]; then
      if [[ "${TASK}" == "create" ]]; then
        create_database
      elif [[ "${TASK}" == "restore" ]]; then
        restore_database
      else
        print_error_and_exit "Unknown task: ${TASK}"
      fi
    fi
    update_state_file 2
  fi

  build_extensions
  build_plugins

  if [[ "${STATE}" -lt 4 ]]; then
    # Configuring i2 Analyze
    if [[ "${STATE}" != 0 ]]; then
      print "Removing Liberty container"
      delete_container "${LIBERTY1_CONTAINER_NAME}"
    elif [[ -n "$(docker ps -aq -f name="^${LIBERTY1_CONTAINER_NAME}$")" ]]; then
      # If liberty exists then clean logs
      clear_liberty_validation_log
    fi

    deploy_liberty

    update_state_file 3
    # Creating a copy of the configuration that was deployed originally
    print_info "Initializing diff tool"
    delete_folder_if_exists_and_create "${PREVIOUS_CONFIGURATION_PATH}"
    update_previous_configuration_with_current

    # Validate Configuration
    liberty_msg=$(check_liberty_status)
    if [[ "${DEPLOYMENT_PATTERN}" == *"store"* ]]; then
      update_match_rules
      download_highlight_queries_xsd
    fi

    update_state_file 4
    echo "${liberty_msg}"
  fi

  local extra_args=()
  if [[ "${VERBOSE}" == "true" ]]; then
    extra_args+=("-v")
  fi
  if [[ "${YES_FLAG}" == "true" ]]; then
    extra_args+=("-y")
  fi

  "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/scripts/create-change-set" \
    -e "${ENVIRONMENT}" -c "${CONFIG_NAME}" -t "initial" -n 0 "${extra_args[@]}"
  update_change_sets_to_new_folder

  print "Deployed Successfully"
  echo "This application is configured for access on ${FRONT_END_URI}"
}

function update_change_sets_to_new_folder() {
  # create-change-sets script will always create a new change set as a copy of the old one then modify accordingly
  delete_folder_if_exists "${LOCAL_USER_CHANGE_SETS_DIR}"
  mv "${NEW_LOCAL_CHANGE_SETS_DIR}" "${LOCAL_USER_CHANGE_SETS_DIR}"
}

function deploy_metrics() {
  if [[ "${DEV_DISABLE_METRICS}" != "true" ]]; then
    # Deploying Prometheus
    delete_container "${PROMETHEUS_CONTAINER_NAME}"
    run_prometheus
    wait_for_prometheus_server_to_be_live

    # Deploying Grafana
    delete_container "${GRAFANA_CONTAINER_NAME}"
    run_grafana
    wait_for_grafana_server_to_be_live
  fi
}

###############################################################################
# Upgrade Helper Functions                                                    #
###############################################################################

function update_config_version() {
  local version="$1"
  sed -i "s/^SUPPORTED_I2ANALYZE_VERSION=.*/SUPPORTED_I2ANALYZE_VERSION=${version}/g" \
    "${ANALYZE_CONTAINERS_ROOT_DIR}/configs/${CONFIG_NAME}/version.conf"
}

function upgrade() {
  wait_for_user_reply "The '${CONFIG_NAME}' config will be upgraded. You cannot revert the upgrade. Are you sure you want to continue?"

  initialize_deployment

  extra_args=()
  if [[ "${VERBOSE}" == "true" ]]; then
    extra_args+=("-v")
  fi
  if [[ "${YES_FLAG}" == "true" ]]; then
    extra_args+=("-y")
  fi
  "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/scripts/create-change-set" \
    -e "${ENVIRONMENT}" -c "${CONFIG_NAME}" -t "upgrade-release" "${extra_args[@]}"

  update_config_version "${CURRENT_SUPPORTED_I2ANALYZE_VERSION}"

  source "${ANALYZE_CONTAINERS_ROOT_DIR}/configs/${CONFIG_NAME}/version.conf"
  source "${ANALYZE_CONTAINERS_ROOT_DIR}/configs/${CONFIG_NAME}/utils/variables.conf"
  source "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/simulated_external_variables.sh"
  source "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/common_variables.sh"
  source "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/internal_helper_variables.sh"

  # Delete old containers
  stop_config_dev_containers
  remove_all_containers_for_the_config "${CONFIG_NAME}"
  remove_docker_volumes

  # Required to fix text search
  # TODO: Look into what specifically is required  to repeat inside this function (CIR-2285)
  initialize_deployment
  run_i2_analyze_tool "/opt/i2-tools/scripts/generateChangeLog.sh"

  validate_json_files

  # Running Solr and ZooKeeper
  deploy_zk_cluster
  configure_zk_for_solr_cluster
  deploy_solr_cluster

  # Configuring Solr and ZooKeeper
  wait_for_solr_to_be_live "${SOLR1_FQDN}"
  configure_solr_collections
  create_solr_collections
  update_state_file 1

  if [[ "${DEPLOYMENT_PATTERN}" == *"store"* ]]; then
    if [[ -z "${BACKUP_NAME}" ]]; then
      BACKUP_NAME="global-upgrade"
    fi
    restore_database
    upgrade_database
  fi

  if [[ "${DEV_DISABLE_METRICS}" != "true" ]]; then
    run_prometheus
    wait_for_prometheus_server_to_be_live

    run_grafana
    wait_for_grafana_server_to_be_live
  fi

  update_state_file 2

  upgrade_connectors

  # Upgrading extensions and plugins need to happen before liberty since they are baked into the image
  upgrade_extensions
  upgrade_plugins
  upgrade_liberty
  update_state_file 4

  update_previous_configuration_with_current
  if [[ "${DEV_DISABLE_METRICS}" != "true" ]]; then
    update_previous_grafana_configuration_with_current
    update_previous_prometheus_configuration_with_current
  fi
  update_change_sets_to_new_folder

  # Copy upgrade-history.json to config
  mv "${LOCAL_CONFIG_DIR}/environment/upgrade-history.json" "${LOCAL_USER_CONFIG_DIR}/environment/upgrade-history.json"

  print "Upgraded Successfully"
  echo "This application is configured for access on ${FRONT_END_URI}"
}

function upgrade_database() {
  print "Upgrading Database"

  case "${DB_DIALECT}" in
  db2)
    echo "DB2 is not supported"
    ;;
  sqlserver)
    run_sql_server_command_as_dba "/opt/databaseScripts/generated/runDatabaseScripts.sh" "/opt/databaseScripts/generated/upgrade"
    ;;
  postgres)
    run_postgres_server_command_as_dba "/opt/databaseScripts/generated/runDatabaseScripts.sh" "/opt/databaseScripts/generated/upgrade"
    ;;
  esac
}

function upgrade_liberty() {
  deploy_liberty
  login_to_liberty
}

function upgrade_extension_pom_xml() {
  local pom_path="$1"
  local template_pom_path="${ANALYZE_CONTAINERS_ROOT_DIR}/templates/extension-development/pom.xml"
  local temp_upgraded_pom_path="${ANALYZE_CONTAINERS_ROOT_DIR}/i2a-extensions/temp-upgraded-pom.xml"
  local groupId artifactId version
  local extension_specific_dependencies
  local extension_specific_dependencies_arr=()

  # Copy the template_pom to the updated_pom
  cp "$template_pom_path" "$temp_upgraded_pom_path"

  # Retrieve groupId, artifactId, and version from the old pom.xml
  groupId=$(xmlstarlet sel -t -v "/project/groupId" "${pom_path}")
  artifactId=$(xmlstarlet sel -t -v "/project/artifactId" "${pom_path}")
  version=$(xmlstarlet sel -t -v "/project/version" "${pom_path}")

  # Update the group ID, artifact ID, and version in the temp_upgraded_pom file
  xmlstarlet ed -L -u "/project/groupId" -v "${groupId}" \
    -u "/project/artifactId" -v "${artifactId}" \
    -u "/project/version" -v "${version}" \
    "$temp_upgraded_pom_path"

  # Retrieves extension specific dependencies
  extension_specific_dependencies=$(xmlstarlet sel -t -c "//*[preceding-sibling::comment()[.=' Extension specific dependencies ']]/*[not(self::comment())]" "$pom_path" | sed 's/<\/version>/<\/version>\n/g')

  if [[ -n "$extension_specific_dependencies" ]]; then
    # Split the extension_specific_dependencies on new lines and populate the extension_specific_dependencies_arr array
    while IFS= read -r line; do
      extension_specific_dependencies_arr+=("$line")
    done <<<"$extension_specific_dependencies"

    # Insert each dependency into the temp_upgraded_pom file
    for dependency in "${extension_specific_dependencies[@]}"; do
      # Extract the groupId, artifactId, and version from the dependency using xmlstarlet
      xml_string="<dependency>${dependency}</dependency>"
      groupId=$(xmlstarlet sel -t -v "//dependency/groupId" <<<"$xml_string")
      artifactId=$(xmlstarlet sel -t -v "//dependency/artifactId" <<<"$xml_string")
      version=$(xmlstarlet sel -t -v "//dependency/version" <<<"$xml_string")

      # Insert the dependency into the temp_upgraded_pom file
      xmlstarlet ed -L -s "/project/dependencies" -t elem -n "dependency" \
        -s "//dependency[last()]" -t elem -n "groupId" -v "$groupId" \
        -s "//dependency[last()]" -t elem -n "artifactId" -v "$artifactId" \
        -s "//dependency[last()]" -t elem -n "version" -v "$version" \
        "$temp_upgraded_pom_path"
    done
  fi

  mv "${temp_upgraded_pom_path}" "${pom_path}"
}

function upgrade_extensions() {
  local extension_references_file="${LOCAL_USER_CONFIG_DIR}/extension-references.json"
  local extension_dependencies_path="${EXTENSIONS_DIR}/extension-dependencies.json"
  local extension_names

  print "Upgrading Extensions"

  readarray -t extension_names < <(jq -r '.extensions[] | .name' <"${extension_references_file}")
  for extension in "${extension_names[@]}"; do
    IFS=' ' read -ra dependencies <<<"$(jq -r --arg name "${extension}" '.[] | select(.name == $name) | .dependencies[]' "${extension_dependencies_path}" | xargs)"
    for dependency in "${dependencies[@]}"; do
      if ! is_string_in_array "${dependency}" extension_names; then
        extension_names+=("${dependencies[@]}")
      fi
    done
  done

  for extension_name in "${extension_names[@]}"; do
    upgrade_extension_pom_xml "${ANALYZE_CONTAINERS_ROOT_DIR}/i2a-extensions/${extension_name}/pom.xml"
    delete_file_if_exists "${PREVIOUS_EXTENSIONS_DIR}/${extension_name}.sha512"
  done

  build_extensions
}

function upgrade_plugins() {
  print "Upgrading Plugins"
  build_plugins
}

function upgrade_connectors() {
  print "Upgrading Connectors"

  # Override Dockerfile with latest template
  for connector_path in "${CONNECTOR_IMAGES_DIR}"/*; do
    if [[ -d "${connector_path}" ]]; then
      connector_type=$(jq -r '.type' <"${connector_path}/connector-definition.json")

      # Upgrade Dockerfile
      if [[ "${connector_type}" == "${I2CONNECT_SERVER_CONNECTOR_TYPE}" ]]; then
        cp "${ANALYZE_CONTAINERS_ROOT_DIR}/templates/i2connect-server-connector-image/Dockerfile" "${connector_path}/Dockerfile"
      elif grep -q "FROM adoptopenjdk/openjdk" "${connector_path}/Dockerfile"; then
        cp "${ANALYZE_CONTAINERS_ROOT_DIR}/templates/springboot-connector-image/Dockerfile" "${connector_path}/Dockerfile"
      elif grep -q "FROM registry.access.redhat.com/ubi8/nodejs" "${connector_path}/Dockerfile"; then
        cp "${ANALYZE_CONTAINERS_ROOT_DIR}/templates/node-connector-image/Dockerfile" "${connector_path}/Dockerfile"
      fi
    fi
  done
  build_connectors
  start_connectors_for_config
}

###############################################################################
# Update Helper Functions                                                     #
###############################################################################

function notify_update_server_configuration() {
  print_info "Updating server configuration on i2 Analyze Application"
  if curl \
    -L --max-redirs 5 -w "%{http_code}" \
    -s -o "/tmp/response.txt" \
    --cacert "${LOCAL_EXTERNAL_CA_CERT_DIR}/CA.cer" \
    --cookie /tmp/cookie.txt \
    --header 'Content-Type: application/json' \
    --data-raw "{\"params\":[{\"value\" : [\"\"],\"type\" : {\"className\":\"java.util.ArrayList\",\"items\":[\"java.lang.String\"]}},{\"value\" : [\"/opt/ol/wlp/usr/shared/config/user.registry.xml\"],\"type\" : {\"className\":\"java.util.ArrayList\",\"items\":[\"java.lang.String\"]}},{\"value\" : [\"\"],\"type\" : {\"className\":\"java.util.ArrayList\",\"items\":[\"java.lang.String\"]}}],\"signature\":[\"java.util.Collection\",\"java.util.Collection\",\"java.util.Collection\"]}" \
    --request POST "${BASE_URI}/IBMJMXConnectorREST/mbeans/WebSphere%3Aservice%3Dcom.ibm.ws.kernel.filemonitor.FileNotificationMBean/operations/notifyFileChanges" \
    >/tmp/http_code.txt; then
    # Invoking FileNotificationMBean doc: https://www.ibm.com/docs/en/zosconnect/3.0?topic=demand-invoking-filenotificationmbean-from-rest-api
    http_code=$(cat /tmp/http_code.txt)
    if [[ "${http_code}" != 200 ]]; then
      print_error_and_exit "Problem updating server configuration application. Returned:${http_code}"
    else
      print_info "Response from i2 Analyze Web UI:$(cat /tmp/response.txt)"
    fi
  else
    print_error_and_exit "Problem calling curl:$(cat /tmp/http_code.txt)"
  fi
}

function update_live_configuration() {
  local errors_message="Validation errors detected, please review the above message(s)."
  local success_message="No Validation errors detected."
  if [[ " ${files_changed_array[*]} " == *" user.registry.xml "* ]]; then
    notify_update_server_configuration
    return
  fi

  local max_tries=15

  print "Calling reload live configuration endpoint"
  for i in $(seq 1 "${max_tries}"); do
    if curl \
      -L --max-redirs 5 \
      -s -o /tmp/response.txt -w "%{http_code}" \
      --cookie /tmp/cookie.txt \
      --cacert "${LOCAL_EXTERNAL_CA_CERT_DIR}/CA.cer" \
      --header "Origin: ${FRONT_END_URI}" \
      --header 'Content-Type: application/json' \
      --request POST "${FRONT_END_URI}/api/v1/admin/config/reload" >/tmp/http_code.txt; then
      http_code=$(cat /tmp/http_code.txt)
      if [[ "${http_code}" == 200 ]]; then
        # sed '/^\s*$/d' will remove empty line
        validation_messages=$(docker exec "${LIBERTY1_CONTAINER_NAME}" cat "/logs/opal-services/i2_Validation.log" | sed '/^\s*$/d')
        if [[ -n "${validation_messages}" ]]; then
          if [[ "${DEV_BUILD}" != "true" ]]; then
            echo "${validation_messages}" >&2
            print_error_and_exit "${errors_message}"
          else
            local unexpected_warn_found=false
            local expected_warn_message="InfoStore consistency validation is disabled, skipping validation..."
            # check for unexpected errors line by line
            while IFS= read -r line; do
              if [[ "${line}" != *"${expected_warn_message}"* ]]; then
                unexpected_warn_found=true
                break
              fi
            done <<<"${validation_messages}"

            if [[ "${unexpected_warn_found}" == true ]]; then
              echo "${validation_messages}" >&2
              print_error_and_exit "${errors_message}"
            else
              echo "${success_message}"
              return 0
            fi
          fi
        else
          echo "${success_message}"
          return 0
        fi
      fi
    fi

    echo "Problem calling reload live configuration. Waiting..."
    sleep 5
  done

  jq --raw-output '.errors[].message' /tmp/response.txt
  print_error_and_exit "${errors_message}"
}

function call_gateway_reload() {
  local errors_message="Validation errors detected, please review the above message(s)."

  if [[ "${RELOAD_GATEWAY_REQUIRED}" == "true" ]]; then
    print "Calling gateway reload endpoint"
    login_to_liberty

    local max_tries=20

    for i in $(seq 1 "${max_tries}"); do
      if curl \
        -L --max-redirs 5 \
        -s -o /tmp/response.txt -w "%{http_code}" \
        --cookie /tmp/cookie.txt \
        --cacert "${LOCAL_EXTERNAL_CA_CERT_DIR}/CA.cer" \
        --header "Origin: ${FRONT_END_URI}" \
        --header 'Content-Type: application/json' \
        --request POST "${FRONT_END_URI}/api/v1/gateway/reload" >/tmp/http_code.txt; then
        http_code=$(cat /tmp/http_code.txt)
        if [[ "${http_code}" == 200 ]]; then
          echo "No Validation errors detected."
          return 0
        fi
      fi

      echo "Problem calling reload gateway. Waiting..."
      sleep 5
    done

    print_info "http_code: ${http_code}"
    print_info "response: $(</tmp/response.txt)"
    print_error_and_exit "${errors_message}"
  fi
}

function login_to_liberty() {
  local max_tries=10
  local app_admin_password

  app_admin_password=$(get_application_admin_password)

  print_info "Getting Auth cookie"
  for i in $(seq 1 "${max_tries}"); do
    # Don't follow redirects on this curl command since we expect login as 302 (redirect status code)
    if curl \
      -s -o /tmp/response.txt -w "%{http_code}" \
      --cookie-jar /tmp/cookie.txt \
      --cacert "${LOCAL_EXTERNAL_CA_CERT_DIR}/CA.cer" \
      --request POST "${BASE_URI}/IBMJMXConnectorREST/j_security_check" \
      --header "Origin: ${BASE_URI}" \
      --header 'Content-Type: application/x-www-form-urlencoded' \
      --data-urlencode "j_username=${I2_ANALYZE_ADMIN}" \
      --data-urlencode "j_password=${app_admin_password}" >/tmp/http_code.txt; then
      http_code=$(cat /tmp/http_code.txt)
      if [[ "${http_code}" == 302 ]]; then
        echo "Logged in to Liberty server" && return 0
      else
        print_info "Failed login with status code:${http_code}"
      fi
    fi
    echo "Liberty is NOT live (attempt: $i). Waiting..."
    sleep 5
  done
  print_info "Liberty won't start- resetting"
  update_state_file 2
  print_error_and_exit "Could not authenticate with Liberty- please try again"
}

function control_application() {
  local operation="$1"
  local current_liberty_mem_perc
  print_info "Running '${operation}' on i2 Analyze Application"

  # Workaround for bug #CIR-1650. This shouldn't be needed after 4.4.0.1 fix pack
  current_liberty_mem_perc="$(get_liberty_mem_perc)"
  if (($(echo "${current_liberty_mem_perc} > 60" | bc -l))); then
    restart_server
    wait_for_liberty_to_be_live "true"
    if [[ "${operation}" != "stop" ]]; then
      return
    fi
  fi

  if curl \
    -L --max-redirs 5 \
    -s -o "/tmp/response.txt" -w "%{http_code}" \
    --cacert "${LOCAL_EXTERNAL_CA_CERT_DIR}/CA.cer" \
    --cookie /tmp/cookie.txt \
    --header 'Content-Type: application/json' \
    --data-raw '{}' \
    --request POST "${BASE_URI}/IBMJMXConnectorREST/mbeans/WebSphere%3Aname%3Dopal-services%2Cservice%3Dcom.ibm.websphere.application.ApplicationMBean/operations/${operation}" \
    >/tmp/http_code.txt; then
    http_code=$(cat /tmp/http_code.txt)
    if [[ "${http_code}" != 200 ]]; then
      print_error_and_exit "Problem running '${operation}' application. Returned:${http_code}"
    else
      print_info "Response from i2 Analyze Web UI:$(cat /tmp/response.txt)"
    fi
  else
    print_error_and_exit "Problem calling curl:$(cat /tmp/http_code.txt)"
  fi
}

function restart_application() {
  control_application restart
}

function stop_application() {
  control_application stop
}

function start_application() {
  control_application start
}

function restart_server() {
  # We can't use /liberty/bin/server stop defaultServer because the container process will stop
  # which will stop the container
  docker restart "${LIBERTY1_CONTAINER_NAME}"
}

function get_liberty_mem_perc() {
  docker stats "${LIBERTY1_CONTAINER_NAME}" --no-stream --format "{{.MemPerc}}" | tr -d '%'
}

function copy_local_config_to_the_liberty_container() {
  local liberty_server_path="/config"
  local liberty_app_war_path="${liberty_server_path}/apps/opal-services.war"
  print_info "Copying configuration to the Liberty container (${LIBERTY1_CONTAINER_NAME})"

  # All other configuration is copied to the application WEB-INF/classes directory.
  local tmp_classes_dir="${ANALYZE_CONTAINERS_ROOT_DIR}/.tmp_classes"
  create_folder "${tmp_classes_dir}"
  find "${GENERATED_LOCAL_CONFIG_DIR}" -maxdepth 1 -type f ! -name user.registry.xml ! -name extension-references.json ! -name plugin-references.json ! -name connector-references.json ! -name '*.xsd' ! -name server.xml ! -name server.extensions.xml ! -name server.extensions.dev.xml -exec cp -t "${tmp_classes_dir}" {} \;

  # In the schema_dev deployment point Gateway schemes to the ISTORE schemes
  if [[ "${DEPLOYMENT_PATTERN}" == "schema_dev" ]]; then
    sed -i 's/^SchemaResource=/Gateway.External.SchemaResource=/' "${tmp_classes_dir}/ApolloServerSettingsMandatory.properties"
    sed -i 's/^ChartingSchemesResource=/Gateway.External.ChartingSchemesResource=/' "${tmp_classes_dir}/ApolloServerSettingsMandatory.properties"
  fi

  docker cp "${tmp_classes_dir}/." "${LIBERTY1_CONTAINER_NAME}:${liberty_app_war_path}/WEB-INF/classes"
  if [[ -f "${GENERATED_LOCAL_CONFIG_DIR}/server.extensions.xml" ]]; then
    docker cp "${GENERATED_LOCAL_CONFIG_DIR}/server.extensions.xml" "${LIBERTY1_CONTAINER_NAME}:${liberty_server_path}"
  fi
  if [[ -f "${GENERATED_LOCAL_CONFIG_DIR}/server.extensions.dev.xml" ]]; then
    docker cp "${GENERATED_LOCAL_CONFIG_DIR}/server.extensions.dev.xml" "${LIBERTY1_CONTAINER_NAME}:${liberty_server_path}"
  fi
  if [[ -f "${GENERATED_LOCAL_CONFIG_DIR}/web.xml" ]]; then
    docker cp "${GENERATED_LOCAL_CONFIG_DIR}/web.xml" "${LIBERTY1_CONTAINER_NAME}:${liberty_app_war_path}/WEB-INF"
  fi
  if [[ -f "${GENERATED_LOCAL_CONFIG_DIR}/jvm.options" ]]; then
    docker cp "${GENERATED_LOCAL_CONFIG_DIR}/jvm.options" "${LIBERTY1_CONTAINER_NAME}:${liberty_server_path}"
  fi
  rm -rf "${tmp_classes_dir}"

  docker cp "${GENERATED_LOCAL_CONFIG_DIR}/user.registry.xml" "${LIBERTY1_CONTAINER_NAME}:/liberty/usr/shared/config"
  docker cp "${GENERATED_LOCAL_CONFIG_DIR}/server.xml" "${LIBERTY1_CONTAINER_NAME}:${liberty_server_path}"
  docker cp "${GENERATED_LOCAL_CONFIG_DIR}/web-dir-extensions/." "${LIBERTY1_CONTAINER_NAME}:${liberty_app_war_path}"

  connector_url_map_new=$(cat "${CONNECTOR_IMAGES_DIR}"/connector-url-mappings-file.json)

  docker exec "${LIBERTY1_CONTAINER_NAME}" bash -c "export CONNECTOR_URL_MAP='${connector_url_map_new}'; \
    rm ${liberty_app_war_path}/WEB-INF/classes/connectors.json; \
    rm ${liberty_app_war_path}/already_run; \
    /opt/entrypoint.d/create-connector-config"
}

function update_previous_prometheus_configuration_with_current() {
  print_info "Copying Prometheus configuration from (${CURRENT_CONFIGURATION_PATH}) to (${PREVIOUS_CONFIGURATION_PATH})"
  cp -pR "${CURRENT_CONFIGURATION_PATH}/prometheus"/* "${PREVIOUS_CONFIGURATION_PATH}/prometheus"
}

function update_previous_grafana_configuration_with_current() {
  print_info "Copying Grafana configuration from (${CURRENT_CONFIGURATION_PATH}) to (${PREVIOUS_CONFIGURATION_PATH})"
  cp -pR "${CURRENT_CONFIGURATION_PATH}/grafana/dashboards/"* "${PREVIOUS_CONFIGURATION_PATH}/grafana/dashboards"
}

function update_previous_configuration_with_current() {
  print_info "Copying configuration from (${CURRENT_CONFIGURATION_PATH}) to (${PREVIOUS_CONFIGURATION_PATH})"
  cp -pR "${CURRENT_CONFIGURATION_PATH}"/* "${PREVIOUS_CONFIGURATION_PATH}"
  # Add references files to the previous config
  cp "${LOCAL_USER_CONFIG_DIR}/plugin-references.json" "${PREVIOUS_CONFIGURATION_PATH}"
  cp "${LOCAL_USER_CONFIG_DIR}/connector-references.json" "${PREVIOUS_CONFIGURATION_PATH}"
  cp "${LOCAL_USER_CONFIG_DIR}/extension-references.json" "${PREVIOUS_CONFIGURATION_PATH}"
  create_folder "${PREVIOUS_CONFIGURATION_UTILS_PATH}"
  cp -p "${CURRENT_CONFIGURATION_UTILS_PATH}/variables.conf" "${PREVIOUS_CONFIGURATION_UTILS_PATH}/variables.conf"
}

function update_match_rules() {
  print "Updating system match rules"

  print_info "Uploading system match rules"
  run_i2_analyze_tool "/opt/i2-tools/scripts/runIndexCommand.sh" "update_match_rules"

  print_info "Waiting for the standby match index to complete indexing"
  local stand_by_match_index_ready_file_path="/logs/StandbyMatchIndexReady"
  while docker exec "${LIBERTY1_CONTAINER_NAME}" test ! -f "${stand_by_match_index_ready_file_path}"; do
    print_info "waiting..."
    sleep 3
  done

  print "Switching standby match index to live"
  run_i2_analyze_tool "/opt/i2-tools/scripts/runIndexCommand.sh" switch_standby_match_index_to_live

  print_info "Removing StandbyMatchIndexReady file from the liberty container"
  docker exec "${LIBERTY1_CONTAINER_NAME}" bash -c "rm ${stand_by_match_index_ready_file_path} > /dev/null 2>&1"
}

function rebuild_database() {
  wait_for_user_reply "Do you wish to rebuild the ISTORE database? This will permanently remove data from the deployment."
  case "${DB_DIALECT}" in
  db2)
    print_info "Removing existing Db2 Server container"
    delete_container "${DB2_SERVER_CONTAINER_NAME}"
    docker volume rm -f "${DB2_SERVER_VOLUME_NAME}" "${DB2_SERVER_BACKUP_VOLUME_NAME}"
    initialize_db2_server
    ;;
  sqlserver)
    print_info "Removing existing SQL Server container"
    delete_container "${SQL_SERVER_CONTAINER_NAME}"
    docker volume rm -f "${SQL_SERVER_VOLUME_NAME}" "${SQL_SERVER_BACKUP_VOLUME_NAME}"
    initialize_sql_server
    ;;
  postgres)
    print_info "Removing existing Postgres container"
    delete_container "${POSTGRES_SERVER_CONTAINER_NAME}"
    docker volume rm -f "${POSTGRES_SERVER_VOLUME_NAME}" "${POSTGRES_SERVER_BACKUP_VOLUME_NAME}"
    initialize_postgres_server
    ;;
  esac
}

function download_highlight_queries_xsd() {
  local app_admin_password
  app_admin_password=$(get_application_admin_password)

  print_info "Downloading highlight-queries-configuration.xsd"
  run_i2_analyze_tool_as_external_user bash -c "curl \
      --silent \
      --cookie-jar /tmp/cookie.txt \
      --cacert /tmp/i2acerts/CA.cer \
      --request POST \"${FRONT_END_URI}/j_security_check\" \
      --header 'Origin: ${FRONT_END_URI}' \
      --header 'Content-Type: application/x-www-form-urlencoded' \
      --data-urlencode 'j_username=${I2_ANALYZE_ADMIN}' \
      --data-urlencode 'j_password=${app_admin_password}' \
    && curl \
      --silent \
      --cookie /tmp/cookie.txt \
      --cacert /tmp/i2acerts/CA.cer \
      --header 'Origin: ${FRONT_END_URI}' \
      --header 'Content-Type: application/xml' \
      \"${FRONT_END_URI}/api/v1/admin/highlightqueryconfig/xsd\"" >"${LOCAL_USER_CONFIG_DIR}/highlight-queries-configuration.xsd"
}

function update_schema() {
  local errors_message="Validation errors detected, please review the above message(s)"
  print "Updating the deployed schema"

  if [[ "${DEPLOYMENT_PATTERN}" == *"store"* && "${IS_DESTRUCTIVE_CHANGE}" == "false" ]]; then
    print_info "Stopping Liberty container"
    stop_application

    if [[ "${CHANGE_SET_EXISTS}" == "false" ]]; then
      local extra_args=()
      if [[ "${VERBOSE}" == "true" ]]; then
        extra_args+=("-v")
      fi
      if [[ "${YES_FLAG}" == "true" ]]; then
        extra_args+=("-y")
      fi
      "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/scripts/create-change-set" \
        -e "${ENVIRONMENT}" -c "${CONFIG_NAME}" -t "additive" "${extra_args[@]}"
      CHANGE_SET_EXISTS="true"
      run_destructive_change_if_destructive_change_set
    fi

    if [[ "${IS_DESTRUCTIVE_CHANGE}" == "false" && -d "${LOCAL_GENERATED_DIR}/update" && "$(ls -A "${LOCAL_GENERATED_DIR}/update")" ]]; then
      print "Running the generated scripts"
      case "${DB_DIALECT}" in
      db2)
        run_db2_server_command_as_db2inst1 "/opt/databaseScripts/generated/runDatabaseScripts.sh" "/opt/databaseScripts/generated/update"
        ;;
      sqlserver)
        run_sql_server_command_as_dba "/opt/databaseScripts/generated/runDatabaseScripts.sh" "/opt/databaseScripts/generated/update"
        ;;
      postgres)
        run_postgres_server_command_as_dba "/opt/databaseScripts/generated/runDatabaseScripts.sh" "/opt/databaseScripts/generated/update"
        ;;
      esac
    fi
  fi
}

function update_security_schema() {
  local errors_message="Validation errors detected, please review the above message(s)"

  if [[ "${DEPLOYMENT_PATTERN}" == *"store"* && "${IS_DESTRUCTIVE_CHANGE}" == "false" ]]; then
    print_info "Stopping Liberty application"
    stop_application

    if [[ "${CHANGE_SET_EXISTS}" == "false" ]]; then
      local extra_args=()
      if [[ "${VERBOSE}" == "true" ]]; then
        extra_args+=("-v")
      fi
      if [[ "${YES_FLAG}" == "true" ]]; then
        extra_args+=("-y")
      fi
      "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/scripts/create-change-set" \
        -e "${ENVIRONMENT}" -c "${CONFIG_NAME}" -t "additive" "${extra_args[@]}"
      CHANGE_SET_EXISTS="true"
      run_destructive_change_if_destructive_change_set
    fi

    if [[ "${IS_DESTRUCTIVE_CHANGE}" == "false" ]]; then
      print "Updating the deployed security schema"
      if ! run_i2_analyze_tool "/opt/i2-tools/scripts/updateSecuritySchema.sh"; then
        print_error_and_exit "${errors_message}"
      fi
    fi
  fi
}

function solr_schema_change() {
  if [[ "${DEPLOYMENT_PATTERN}" == *"store"* ]]; then
    print_info "Stopping Liberty application"
    stop_application
  fi

  print_info "Updating the deployed Solr indexes"
  print_warn "Destructive Solr indexes change(s) detected"
  wait_for_user_reply "The Solr configuration changes require a re-index. Do you wish to rebuild the Solr indexes?"
  configure_solr_collections
  clear_search_index
  print_info "Destructive Solr indexes change(s) complete"
}

function restart_solr_node() {
  stop_container "${SOLR1_CONTAINER_NAME}"
  start_container "${SOLR1_CONTAINER_NAME}"
  wait_for_solr_to_be_live "${SOLR1_FQDN}"
}

function update_solr_jars_for_dev() {
  if [[ "${DEV_BUILD}" == "true" ]]; then
    local extension_references_file="${LOCAL_USER_CONFIG_DIR}/extension-references.json"
    local extension_files
    readarray -t extension_files < <(jq -r '.extensions[] | .name' <"${extension_references_file}")
    local solr_jars_updated="false"
    local solr_jars=("ApolloLegacy.jar" "DiscoUtils.jar" "SolrCommon.jar" "SolrExtension.jar")
    local solr_jars_container_path="/opt/i2-plugin/lib"
    for extension_name in "${extension_files[@]}"; do
      if [[ ! -f "${EXTENSIONS_DIR}/${extension_name}/pom.xml" ]]; then
        while read -r file_name; do
          if [[ -f "${file_name}" ]]; then
            if [[ " ${solr_jars[*]} " == *" ${file_name##*/} "* ]]; then
              docker cp "${file_name}" "${SOLR1_CONTAINER_NAME}:${solr_jars_container_path}"
              solr_jars_updated="true"
            fi
          fi
        done <<<"$(find -L "${EXTENSIONS_DIR}/${extension_name}" -mindepth 1 -maxdepth 1 -type f -name "*.jar")"
      fi
    done
    if [[ "${solr_jars_updated}" == "true" ]]; then
      restart_solr_node
    fi
  fi
}

function solr_synonyms_change() {
  print_info "Updating the deployed Solr synonyms"

  configure_solr_collections
  restart_solr_node
}

function run_destructive_change_if_destructive_change_set() {
  last_change_set_folder=$(find -L "${NEW_LOCAL_CHANGE_SETS_DIR}" -mindepth 1 -maxdepth 1 -type d -printf '%f\n' | sort -V | tail -n 1)
  if [[ "${last_change_set_folder}" == *"-destructive-"* ]]; then
    destructive_schema_or_security_schema_change
  fi
}

function destructive_schema_or_security_schema_change() {
  local release_change_set_path release_change_set_name
  print_warn "Destructive Security Schema change(s) detected"
  IS_DESTRUCTIVE_CHANGE="true"

  rebuild_database
  delete_solr_collections
  create_solr_collections
  release_change_set_path="$(find "${NEW_LOCAL_CHANGE_SETS_DIR}" \
    -mindepth 1 -maxdepth 1 -type d -name "1-release*")"
  release_change_set_name="$(basename "${release_change_set_path}")"
  BACKUP_NAME="${release_change_set_name}"
  create_folder "${BACKUP_DIR}/${BACKUP_NAME}"
  backup_database
  print_info "Destructive Security Schema change(s) complete"
}

function update_data_source_id_file() {
  local tmp_dir="/tmp"
  create_data_source_properties "${tmp_dir}"

  docker cp "${tmp_dir}/DataSource.properties" "${LIBERTY1_CONTAINER_NAME}:/config/apps/opal-services.war/WEB-INF/classes"
}

function handle_configuration_change() {
  compare_current_configuration
  compare_current_web_dir_extensions
  compare_current_extensions
  compare_current_plugins

  print_info "Results in array '${files_changed_array[*]}'"

  if [[ "${config_final_action_code}" == 0 ]]; then
    print_info "No updates to the configuration"
    return
  fi

  # Make sure update will be re-run if anything fails
  update_state_file 3
  for fileName in "${files_changed_array[@]}"; do
    if [[ "${fileName}" == "system-match-rules.xml" ]]; then
      if [[ "${DEPLOYMENT_PATTERN}" == *"store"* ]]; then
        update_match_rules
      fi
    elif [[ "${fileName}" == "schema.xml" ]]; then
      update_schema
    elif [[ "${fileName}" == "security-schema.xml" ]]; then
      update_security_schema
    elif [[ "${fileName}" == "environment/dsid/dsid.properties" ]]; then
      update_data_source_id_file
      if [[ "${DEPLOYMENT_PATTERN}" == *"store"* ]]; then
        destructive_schema_or_security_schema_change
      fi
    elif [[ "${fileName}" == "web-dir-extensions" ]]; then
      handle_web_dir_extensions_change
    elif [[ "${fileName}" == "extension-references.json" || "${fileName}" = lib/*.sha512 ]]; then
      handle_extension_change
    elif [[ "${fileName}" == "plugin-references.json" || "${fileName}" = plugins/*.sha512 ]]; then
      handle_plugin_change
    elif [[ "${fileName}" == "solr/schema-template-definition.xml" ]]; then
      solr_schema_change
    elif [[ "${fileName}" == solr/*.txt ]]; then
      if ! is_string_in_array "solr/schema-template-definition.xml" files_changed_array; then
        solr_synonyms_change
      fi
    fi
  done

  if [[ "${config_final_action_code}" == 3 ]] && [[ "${DEPLOYMENT_PATTERN}" == *"store"* ]]; then
    print_info "Starting i2 Analyze application"
    start_application

    print_info "Validating database consistency"
    run_i2_analyze_tool "/opt/i2-tools/scripts/dbConsistencyCheckScript.sh"
  fi

  clear_liberty_validation_log
  copy_local_config_to_the_liberty_container
  case "${config_final_action_code}" in
  1)
    update_live_configuration
    ;;
  2) ;&
    # Fallthrough
  3)
    print_info "Restarting i2 Analyze application"
    if [[ " ${files_changed_array[*]} " == *" user.registry.xml "* ]]; then
      notify_update_server_configuration
    fi
    restart_application
    check_liberty_status
    ;;
  4)
    print_info "Restarting server"
    restart_server
    check_liberty_status "true"
    ;;
  esac

  if [[ " ${files_changed_array[*]} " == *" schema.xml "* && "${DEPLOYMENT_PATTERN}" == *"store"* ]]; then
    download_highlight_queries_xsd
  fi
}

function handle_deployment_pattern_change() {
  print_info "Checking if DEPLOYMENT_PATTERN changed"

  if [[ "${CURRENT_DEPLOYMENT_PATTERN}" != "${PREVIOUS_DEPLOYMENT_PATTERN}" ]]; then
    print "DEPLOYMENT_PATTERN is changed"
    echo "Previous DEPLOYMENT_PATTERN: ${PREVIOUS_DEPLOYMENT_PATTERN}"
    echo "New DEPLOYMENT_PATTERN: ${CURRENT_DEPLOYMENT_PATTERN}"

    print "Removing Liberty container"
    delete_container "${LIBERTY1_CONTAINER_NAME}"
    deploy_liberty
    login_to_liberty

    if [[ "${PREVIOUS_DEPLOYMENT_PATTERN}" == *"store"* ]] && [[ "${CURRENT_DEPLOYMENT_PATTERN}" != *"store"* ]]; then
      stop_db_server
    fi

    if [[ "${CURRENT_DEPLOYMENT_PATTERN}" == *"store"* ]] && [[ "${PREVIOUS_DEPLOYMENT_PATTERN}" != *"store"* ]]; then
      handle_db_initiation_on_pattern_change
    fi
    clear_liberty_validation_log
    stop_application
    start_application
    check_liberty_status
  else
    print_info "DEPLOYMENT_PATTERN is unchanged: ${CURRENT_DEPLOYMENT_PATTERN}"
  fi
}

function handle_dialect_change() {
  print_info "Checking if DB_DIALECT changed"

  if [[ "${CURRENT_DEPLOYMENT_PATTERN}" == *"store"* && "${DB_DIALECT}" != "${PREVIOUS_DB_DIALECT}" && "${PREVIOUS_DEPLOYMENT_PATTERN}" == "${CURRENT_DEPLOYMENT_PATTERN}" ]]; then
    print "DB_DIALECT is changed"
    echo "Previous DB_DIALECT: ${PREVIOUS_DB_DIALECT}"
    echo "New DB_DIALECT: ${DB_DIALECT}"

    clear_search_index

    print "Removing Liberty container"
    delete_container "${LIBERTY1_CONTAINER_NAME}"
    deploy_liberty
    login_to_liberty

    if [[ "${PREVIOUS_DEPLOYMENT_PATTERN}" == *"store"* ]]; then
      stop_db_server "${PREVIOUS_DB_DIALECT}"
    fi

    handle_db_initiation_on_pattern_change

    clear_liberty_validation_log
    stop_application
    start_application
    check_liberty_status
  else
    print_info "DB_DIALECT is unchanged: ${DB_DIALECT}"
  fi
}

function handle_web_dir_extensions_change() {
  local web_liberty_path="/config/apps/opal-services.war"
  local extension_dir
  print "Updating web directory extensions"

  while read -r path; do
    extension_dir="${path#"${PREVIOUS_CONFIGURATION_PATH}/web-dir-extensions"}"
    print_info "Delete old web directory extension ${extension_dir}"
    docker exec "${LIBERTY1_CONTAINER_NAME}" bash -c "rm -rf ${web_liberty_path}/${extension_dir} > /dev/null" || true
  done <<<"$(find "${PREVIOUS_CONFIGURATION_PATH}/web-dir-extensions" -maxdepth 1 -mindepth 1)"

  # The update to these files will be done as part of the copy_local_config_to_the_liberty_container
  # so it isn't needed here
}

function handle_extension_change() {
  local jars_liberty_path="/config/apps/opal-services.war/WEB-INF/lib"
  local extension_references_file="${LOCAL_USER_CONFIG_DIR}/extension-references.json"
  local old_extension_references_file="${PREVIOUS_CONFIGURATION_PATH}/extension-references.json"
  local extension_dependencies_path="${EXTENSIONS_DIR}/extension-dependencies.json"
  local extension_files old_extension_files deleted_extensions
  local filename

  print "Updating i2 Analyze extensions"
  # Remove extensions that were deleted from extension-references.json
  if [[ -d "${PREVIOUS_CONFIGURATION_PATH}" ]]; then
    readarray -t extension_files < <(jq -r '.extensions[] | .name' <"${extension_references_file}")
    readarray -t old_extension_files < <(jq -r '.extensions[] | .name' <"${old_extension_references_file}")
    # Compute deleted extensions
    IFS=' ' read -r -a deleted_extensions <<<"$(subtract_array_from_array extension_files old_extension_files)"
    # Compute deleted extension dependencies
    for extension_name in "${deleted_extensions[@]}"; do
      IFS=' ' read -ra dependencies <<<"$(jq -r --arg name "${extension_name}" '.[] | select(.name == $name) | .dependencies[]' "${extension_dependencies_path}" | xargs)"
      for dependency in "${dependencies[@]}"; do
        if ! is_string_in_array "${dependency}" deleted_extensions; then
          deleted_extensions+=("${dependencies[@]}")
        fi
      done
    done
    for extension_name in "${deleted_extensions[@]}"; do
      print_info "Delete old library ${jars_liberty_path}/${extension_name}"
      if [[ -f "${EXTENSIONS_DIR}/${extension_name}/pom.xml" ]]; then
        extension_version="$(xmlstarlet sel -t -v "/project/version" "${EXTENSIONS_DIR}/${extension_name}/pom.xml")"
        docker exec "${LIBERTY1_CONTAINER_NAME}" bash -c "rm ${jars_liberty_path}/${extension_name}-${extension_version}.jar > /dev/null" || true
      else
        docker exec "${LIBERTY1_CONTAINER_NAME}" bash -c "rm ${jars_liberty_path}/${extension_name}.jar > /dev/null" || true
      fi
    done
  fi

  # Update extensions
  delete_folder_if_exists_and_create "${PREVIOUS_CONFIGURATION_LIB_PATH}"
  readarray -t extension_files < <(jq -r '.extensions[] | .name' <"${extension_references_file}")
  for extension in "${extension_files[@]}"; do
    IFS=' ' read -ra dependencies <<<"$(jq -r --arg name "${extension}" '.[] | select(.name == $name) | .dependencies[]' "${extension_dependencies_path}" | xargs)"
    for dependency in "${dependencies[@]}"; do
      if ! is_string_in_array "${dependency}" extension_files; then
        extension_files+=("${dependencies[@]}")
      fi
    done
  done
  for extension_name in "${extension_files[@]}"; do
    # Save current sha in the previous config
    cp -pr "${PREVIOUS_EXTENSIONS_DIR}/${extension_name}.sha512" "${PREVIOUS_CONFIGURATION_LIB_PATH}"
    # Copy extension to the liberty container
    if [[ -f "${EXTENSIONS_DIR}/${extension_name}/pom.xml" ]]; then
      extension_version="$(xmlstarlet sel -t -v "/project/version" "${EXTENSIONS_DIR}/${extension_name}/pom.xml")"
      docker cp "${EXTENSIONS_DIR}/${extension_name}/target/${extension_name}-${extension_version}.jar" "${LIBERTY1_CONTAINER_NAME}:${jars_liberty_path}"
    else
      update_solr_jars_for_dev
      while read -r file_name; do
        if [[ -f "${file_name}" ]]; then
          docker cp "${file_name}" "${LIBERTY1_CONTAINER_NAME}:${jars_liberty_path}"
        fi
      done <<<"$(find -L "${EXTENSIONS_DIR}/${extension_name}" -mindepth 1 -maxdepth 1 -type f -name "*.jar")"
    fi
  done
}

function handle_plugin_change() {
  local plugins_liberty_path="/config/apps/opal-services.war/plugins"
  local plugin_references_file="${LOCAL_USER_CONFIG_DIR}/plugin-references.json"
  local old_plugin_references_file="${PREVIOUS_CONFIGURATION_PATH}/plugin-references.json"
  local plugin_files old_plugin_files deleted_plugins
  local filename

  print "Updating i2 Analyze plugins"
  # Remove plugins that were deleted from plugin-references.json
  if [[ -d "${PREVIOUS_CONFIGURATION_PATH}" ]]; then
    readarray -t plugin_files < <(jq -r '.plugins[] | .name' <"${plugin_references_file}")
    readarray -t old_plugin_files < <(jq -r '.plugins[] | .name' <"${old_plugin_references_file}")
    # Compute deleted plugins
    IFS=' ' read -r -a deleted_plugins <<<"$(subtract_array_from_array plugin_files old_plugin_files)"

    for plugin_name in "${deleted_plugins[@]}"; do
      print_info "Delete old plugin ${plugins_liberty_path}/${plugin_name}"
      docker exec "${LIBERTY1_CONTAINER_NAME}" bash -c "rm -rf ${plugins_liberty_path}/${plugin_name} > /dev/null" || true
    done
  fi

  # Update plugins
  delete_folder_if_exists_and_create "${PREVIOUS_CONFIGURATION_PLUGINS_PATH}"
  readarray -t plugin_files < <(jq -r '.plugins[] | .name' <"${plugin_references_file}")
  for plugin_name in "${plugin_files[@]}"; do
    # Save current sha in the previous config
    cp -pr "${PREVIOUS_PLUGINS_DIR}/${plugin_name}.sha512" "${PREVIOUS_CONFIGURATION_PLUGINS_PATH}"
    # Copy plugin to the liberty container
    docker exec "${LIBERTY1_CONTAINER_NAME}" bash -c "mkdir -p ${plugins_liberty_path}/${plugin_name} > /dev/null" || true
    docker cp "${PLUGINS_DIR}/${plugin_name}/." "${LIBERTY1_CONTAINER_NAME}:${plugins_liberty_path}/${plugin_name}"
  done
}

function handle_prometheus_configuration_change() {
  local prometheus_tmp_config_dir="/tmp/prometheus"
  local prometheus_template_file="prometheus.yml"
  local prometheus_password
  local reload_status_code
  compare_current_prometheus_config

  if [[ "${PROMETHEUS_ACTION_CODE}" != 0 ]]; then
    print "Updating prometheus configuration"
    print_info "Copy current Prometheus configuration to Prometheus container"
    update_volume "${LOCAL_PROMETHEUS_CONFIG_DIR}" "${PROMETHEUS_CONFIG_VOLUME_NAME}" "${prometheus_tmp_config_dir}"
    docker exec "${PROMETHEUS_CONTAINER_NAME}" bash -c "/opt/update-prometheus-config.sh"
    update_previous_prometheus_configuration_with_current
    prometheus_password=$(get_prometheus_admin_password)
    reload_status_code=$(run_i2_analyze_tool_as_external_user bash -c "curl --write-out \"%{http_code}\" --silent --output /dev/null \
      -X POST -u ${PROMETHEUS_USERNAME}:${prometheus_password} \
      --cacert /tmp/i2acerts/CA.cer https://${PROMETHEUS_FQDN}:9090/-/reload")
    if [[ "${reload_status_code}" == 200 ]]; then
      echo "Prometheus configuration is updated successfully"
    else
      print_error_and_exit "Prometheus configuration is NOT updated successfully. Unexpected status code: ${reload_status_code}"
    fi
  else
    print_info "No updates to the prometheus configuration"
  fi
}

function handle_grafana_dashboards_change() {
  compare_current_grafana_dashboards

  if [[ "${GRAFANA_ACTION_CODE}" != 0 ]]; then
    print "Updating grafana dashboards"
    print_info "Copy current Grafana dashboards to Grafana container"
    exit_code="$(update_grafana_dashboard_volume)"
    update_previous_grafana_configuration_with_current
    if [[ "${exit_code}" -eq 0 ]]; then
      echo "Grafana dashboards updated successfully"
    else
      print_error_and_exit "Grafana dashboards NOT updated successfully. Unexpected status code: ${exit_code}"
    fi
  else
    print_info "No updates to the grafana configuration"
  fi
}

function handle_connectors_change() {
  build_connectors
  start_connectors_for_config
  call_gateway_reload
}

function update_deployment() {
  initialize_deployment

  # Login to Liberty
  login_to_liberty

  # Handling DEPLOYMENT_PATTERN change
  handle_deployment_pattern_change

  # Handling DB_DIALECT change
  handle_dialect_change

  # Make sure update will be re-run if anything fails
  update_state_file 3

  build_extensions
  build_plugins

  if [[ "${DEV_DISABLE_METRICS}" != "true" ]]; then
    # Handling Prometheus Configuration Change
    handle_prometheus_configuration_change

    # Handling Grafana Configuration Change
    handle_grafana_dashboards_change
  fi

  # Handling Connectors Change
  handle_connectors_change

  # Handling Configuration Change
  handle_configuration_change

  update_state_file 4

  update_previous_configuration_with_current

  if [[ "${CHANGE_SET_EXISTS}" == "true" ]]; then
    update_change_sets_to_new_folder
  fi

  print "Updated Successfully"
  echo "This application is configured for access on ${FRONT_END_URI}"
}

###############################################################################
# Backup Helper Functions                                                     #
###############################################################################

function move_backup_if_exist() {
  local host_backup_file
  local local_backup_file="${BACKUP_DIR}/${BACKUP_NAME}/${DB_BACKUP_FILE_NAME}"

  print "Checking backup does NOT exist"

  # Ensure to canonicalise if it is pointing to a symlink
  # Use option m to ensure missing file doesn't error
  host_backup_file=$(readlink -m "${local_backup_file}")

  if [[ -f "${host_backup_file}" ]]; then
    local old_host_backup_file="${host_backup_file}.bak"
    if [[ "${BACKUP_NAME}" != "default" ]]; then
      wait_for_user_reply "Backup already exist, are you sure you want to overwrite it?"
    fi
    echo "Backup ${host_backup_file} already exists, moving it to ${old_host_backup_file}"
    mv "${host_backup_file}" "${old_host_backup_file}"
  fi
}

function create_backup() {
  initialize_deployment

  wait_for_liberty_to_be_live

  # Check for backup file
  if [[ -z "${BACKUP_NAME}" ]]; then
    print "No backup_name provided, using the 'default' name"
    BACKUP_NAME="default"
  fi

  move_backup_if_exist

  create_folder "${BACKUP_DIR}/${BACKUP_NAME}"

  backup_database
  print_success "Database backup completed"
}

function restore_from_backup() {
  initialize_deployment

  if [[ -z "${BACKUP_NAME}" ]]; then
    print "No backup_name provided, using the 'default' name"
    BACKUP_NAME="default"
  fi
  local backup_file_path="${BACKUP_DIR}/${BACKUP_NAME}/${DB_BACKUP_FILE_NAME}"

  # Validate the backup exists and is not zero bytes before continuing.
  if [[ ! -d "${BACKUP_DIR}/${BACKUP_NAME}" ]]; then
    print_error_and_exit "Backup directory ${BACKUP_DIR}/${BACKUP_NAME} does NOT exist"
  fi
  # Db2 has a timestamp on the file name. Ignore
  if [[ "${DB_DIALECT}" != "db2" && (! -f "${backup_file_path}" || ! -s "${backup_file_path}") ]]; then
    print_error_and_exit "Backup file ${backup_file_path} does NOT exist or is empty."
  fi

  wait_for_user_reply "Are you sure you want to run the 'restore' task? This will permanently remove data from the deployment and restore to the specified backup."
  update_state_file 0
  STATE=0
  create_deployment
}

function check_no_change_set_changes_pending() {
  generate_artifacts

  # Required to ensure comparison works
  add_config_admin
  update_log4j_file

  compare_current_configuration
  if is_string_in_array "schema.xml" files_changed_array || is_string_in_array "security-schema.xml" files_changed_array; then
    print_error_and_exit "There are changes to the schema or security schema files. Please revert the changes before running this task again."
  fi
}

###############################################################################
# Clean Helper Functions                                                      #
###############################################################################

function clean_deployment() {
  check_no_change_set_changes_pending
  wait_for_user_reply "Are you sure you want to run the 'clean' task? This will permanently remove data from the deployment."

  local all_container_ids
  IFS=' ' read -ra all_container_ids <<<"$(docker ps -aq -f network="${DOMAIN_NAME}" -f name="\.${CONFIG_NAME}_${CONTAINER_VERSION_SUFFIX}$" | xargs)"
  print "Deleting all containers for ${CONFIG_NAME} deployment"
  for container_id in "${all_container_ids[@]}"; do
    print_info "Deleting container ${container_id}"
    delete_container "${container_id}"
  done

  remove_docker_volumes

  print_info "Deleting previous configuration folder: ${PREVIOUS_CONFIGURATION_DIR}"
  delete_folder_if_exists "${PREVIOUS_CONFIGURATION_DIR}"
}

###############################################################################
# Connector Helper Functions                                                  #
###############################################################################

function build_connectors() {
  local connector_references_file="${LOCAL_USER_CONFIG_DIR}/connector-references.json"
  local connector_args=()

  if [[ "${YES_FLAG}" == "true" ]]; then
    connector_args+=(-y)
  fi
  if [[ "${VERBOSE}" == "true" ]]; then
    connector_args+=(-v)
  fi

  readarray -t all_connector_names < <(jq -r '.connectors // empty | .[].name' <"${connector_references_file}")

  if [[ "${#all_connector_names}" -gt 0 ]]; then
    print "Building connector images"

    # Only attempt to rebuild connectors for the current config
    for connector_name in "${all_connector_names[@]}"; do
      connector_args+=("-i" "${connector_name}")
    done

    print "Running generate-secrets"
    "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/scripts/generate-secrets" -c connectors "${connector_args[@]}"
    print "Running build-connector-images"
    "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/scripts/build-connector-images" "${connector_args[@]}"
  fi
}

###############################################################################
# Deploy Helper Functions                                                     #
###############################################################################

function initialize_deployment() {
  print_info "Initializing deployment"

  # Validate Configuration
  validate_mandatory_files_present

  # Auto generate dsid into config folder before we create the generated one
  create_data_source_id "${LOCAL_USER_CONFIG_DIR}"

  generate_artifacts

  # Add files that could be missing
  readarray -d '' fileList < <(find "${LOCAL_CONFIGURATION_DIR}" -type f -print0)
  for file in "${fileList[@]}"; do
    filename="${file//"${LOCAL_CONFIGURATION_DIR}/"/}"
    if [[ ! -f "${LOCAL_USER_CONFIG_DIR}/${filename}" ]]; then
      cp "${file}" "${LOCAL_USER_CONFIG_DIR}/${filename}"
    fi
  done

  if [[ "${EXTENSIONS_DEV}" == "true" ]]; then
    add_config_admin
    update_log4j_file
  fi
  create_mounted_config_structure

  if [[ ! -f "${PREVIOUS_STATE_FILE_PATH}" && "${TASK}" != "package" ]]; then
    create_initial_state_file
  fi
}

function validate_json_files() {
  if [[ "${DEV_SKIP_VALIDATION}" == "true" ]]; then
    return
  fi

  local -A json_files_with_schema_map=(
    ["${LOCAL_USER_CONFIG_DIR}/connector-references.json"]="${ANALYZE_CONTAINERS_ROOT_DIR}/utils/schemas/connector-references-schema.json"
    ["${LOCAL_USER_CONFIG_DIR}/extension-references.json"]="${ANALYZE_CONTAINERS_ROOT_DIR}/utils/schemas/extension-references-schema.json"
    ["${LOCAL_USER_CONFIG_DIR}/plugin-references.json"]="${ANALYZE_CONTAINERS_ROOT_DIR}/utils/schemas/plugin-references-schema.json"
    ["${ANALYZE_CONTAINERS_ROOT_DIR}/path-configuration.json"]="${ANALYZE_CONTAINERS_ROOT_DIR}/utils/schemas/path-configuration-schema.json"
    ["${EXTENSIONS_DIR}/extension-dependencies.json"]="${ANALYZE_CONTAINERS_ROOT_DIR}/utils/schemas/extension-dependencies-schema.json"
  )

  for json_file in "${!json_files_with_schema_map[@]}"; do
    if [[ ! -f "${json_file}" ]]; then
      continue
    fi
    if ! check-jsonschema --schemafile "${json_files_with_schema_map["${json_file}"]}" "${json_file}"; then
      print_error_and_exit "Incorrect ${json_file} file, please review the above message(s)."
    fi
  done

  # For connectors
  local connector_references_file="${LOCAL_USER_CONFIG_DIR}/connector-references.json"
  local all_connector_names connector_definition_file connector_version_file
  readarray -t all_connector_names < <(jq -r '.connectors // empty | .[].name' <"${connector_references_file}")

  for connector_name in "${all_connector_names[@]}"; do
    connector_definition_file="${CONNECTOR_IMAGES_DIR}/${connector_name}/connector-definition.json"
    connector_version_file="${CONNECTOR_IMAGES_DIR}/${connector_name}/connector-version.json"
    if [[ -f "${connector_definition_file}" ]] && ! check-jsonschema --schemafile "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/schemas/connector-definition-schema.json" "${connector_definition_file}"; then
      print_error_and_exit "Incorrect ${connector_definition_file} file, please review the above message(s)."
    fi
    if [[ -f "${connector_version_file}" ]] && ! check-jsonschema --schemafile "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/schemas/connector-version-schema.json" "${connector_version_file}"; then
      print_error_and_exit "Incorrect ${connector_version_file} file, please review the above message(s)."
    fi
  done
}

function validate_mandatory_files_present() {
  local mandatory_files=(
    "schema.xml"
    "schema-charting-schemes.xml"
    "security-schema.xml"
    "schema-results-configuration.xml"
    "command-access-control.xml"
    "schema-source-reference-schema.xml"
    "schema-vq-configuration.xml"
    "user.registry.xml"
  )

  if [[ "${DEV_SKIP_VALIDATION}" == "true" ]]; then
    return
  fi

  for mandatory_file in "${mandatory_files[@]}"; do
    if [[ ! -f "${LOCAL_USER_CONFIG_DIR}/${mandatory_file}" ]]; then
      print_error_and_exit "Mandatory file ${mandatory_file} missing from ${LOCAL_USER_CONFIG_DIR}, correct this problem then run deploy again."
    fi
  done

  for xml_file in "${LOCAL_USER_CONFIG_DIR}"/*.xml; do
    if [[ $(head "${xml_file}") == *"<!-- Replace this"* ]]; then
      file_name=$(basename "$xml_file")
      print_error_and_exit "Placeholder text found in ${LOCAL_USER_CONFIG_DIR}/${file_name}, check file contents then run deploy again."
    fi
  done
}

function create_initial_state_file() {
  local template_state_file_path="${ANALYZE_CONTAINERS_ROOT_DIR}/utils/templates/.state.conf"
  print_info "Creating initial ${PREVIOUS_STATE_FILE_PATH} file"
  create_folder "${PREVIOUS_CONFIGURATION_DIR}"
  cp -p "${template_state_file_path}" "${PREVIOUS_STATE_FILE_PATH}"
  STATE=0
}

function work_out_task_to_run() {
  if [[ -f "${PREVIOUS_STATE_FILE_PATH}" ]]; then
    source "${PREVIOUS_STATE_FILE_PATH}"
  else
    STATE=0
  fi
  print_info "STATE: ${STATE}"

  # Only if state is 4 and config versions are different an upgrade should be run
  # Otherwise check state
  source "${ANALYZE_CONTAINERS_ROOT_DIR}/configs/${CONFIG_NAME}/version.conf"
  CONFIG_SUPPORTED_I2ANALYZE_VERSION="${SUPPORTED_I2ANALYZE_VERSION}"
  source "${ANALYZE_CONTAINERS_ROOT_DIR}/version.conf"
  CURRENT_SUPPORTED_I2ANALYZE_VERSION="${SUPPORTED_I2ANALYZE_VERSION}"
  if [[ "${CONFIG_SUPPORTED_I2ANALYZE_VERSION}" != "${CURRENT_SUPPORTED_I2ANALYZE_VERSION}" ]]; then
    if [[ "${CONFIG_SUPPORTED_I2ANALYZE_VERSION}" < "4.3.4.0" ]]; then
      print_error_and_exit "Upgrade from i2 Analyze version ${CONFIG_SUPPORTED_I2ANALYZE_VERSION} is not supported"
    fi
    if [[ "${CONFIG_SUPPORTED_I2ANALYZE_VERSION}" > "${CURRENT_SUPPORTED_I2ANALYZE_VERSION}" ]]; then
      print_error_and_exit "Unsupported i2Analyze version ${CONFIG_SUPPORTED_I2ANALYZE_VERSION}"
    fi
    if [[ "${STATE}" == 4 ]]; then
      print "Upgrading deployment"
      TASK="upgrade"
    else
      print_error_and_exit "The deployment requires to be healthy before upgrading. Please redeploy with i2Analyze version ${CONFIG_SUPPORTED_I2ANALYZE_VERSION} before running this task again."
    fi
  fi

  if [[ "${STATE}" == 0 ]]; then
    print "Creating initial deployment"
    TASK="create"
  elif [[ "${STATE}" == 1 ]] || [[ "${STATE}" == 2 ]]; then
    print "Previous deployment did not complete - retrying"
    TASK="create"
  elif [[ "${STATE}" == 3 ]]; then
    print "Current Deployment is NOT healthy"
    TASK="update"
  elif [[ "${STATE}" == 4 && "${CONFIG_SUPPORTED_I2ANALYZE_VERSION}" == "${CURRENT_SUPPORTED_I2ANALYZE_VERSION}" ]]; then
    print "Updating deployment"
    TASK="update"
  elif [[ "${STATE}" == 5 ]]; then
    print "Recreating deployment with persisted data"
    TASK="recreate"
  fi

  if [[ "${TASK}" == "update" ]]; then
    if ! check_containers_exist; then
      print "Some containers are missing, the deployment is NOT healthy."
      wait_for_user_reply "Do you want to clean and recreate the deployment? This will permanently remove data from the deployment."

      # Reset state
      STATE=0
      TASK="create"
    elif ! check_db_container_exist; then
      print "Some containers are missing, the deployment is NOT healthy."
      wait_for_user_reply "Do you want to clean and recreate the deployment? This will permanently remove data from the deployment."

      clear_search_index
      # Reset state
      STATE=1
      TASK="create"
    elif ! check_connector_containers_exist; then
      # Reset state
      STATE=3
      print_warn "Some connector containers are missing, the deployment is NOT healthy."
    fi
  fi
}

function run_top_level_checks() {
  check_environment_is_valid
  check_deployment_pattern_is_valid
  check_env_vars_are_set
  check_variable_is_set "${HOST_PORT_SOLR}" "HOST_PORT_SOLR environment variable is not set"
  check_variable_is_set "${HOST_PORT_I2ANALYZE_SERVICE}" "HOST_PORT_I2ANALYZE_SERVICE environment variable is not set"
  check_variable_is_set "${HOST_PORT_DB}" "HOST_PORT_DB environment variable is not set"
  if [[ "${DEV_DISABLE_METRICS}" != "true" ]]; then
    check_variable_is_set "${HOST_PORT_PROMETHEUS}" "HOST_PORT_PROMETHEUS environment variable is not set"
    check_variable_is_set "${HOST_PORT_GRAFANA}" "HOST_PORT_GRAFANA environment variable is not set"
  fi
}

function print_deployment_information() {
  print "Deployment Information:"
  echo "ANALYZE_CONTAINERS_ROOT_DIR: ${ANALYZE_CONTAINERS_ROOT_DIR}"
  echo "CONFIG_NAME: ${CONFIG_NAME}"
  echo "DEPLOYMENT_PATTERN: ${DEPLOYMENT_PATTERN}"
  echo "I2A_DEPENDENCIES_IMAGES_TAG: ${I2A_DEPENDENCIES_IMAGES_TAG}"
}

function create_release_change_set() {
  local extra_args=()
  if [[ "${VERBOSE}" == "true" ]]; then
    extra_args+=("-v")
  fi
  if [[ "${YES_FLAG}" == "true" ]]; then
    extra_args+=("-y")
  fi

  check_no_change_set_changes_pending
  "${ANALYZE_CONTAINERS_ROOT_DIR}/utils/scripts/create-change-set" \
    -e "${ENVIRONMENT}" -c "${CONFIG_NAME}" -t "release" "${extra_args[@]}"
  update_change_sets_to_new_folder
}

function validate_release_number() {
  # Find the latest upgrade release
  local latest_upgrade_release=""
  latest_upgrade_release=$(find -L "${LOCAL_USER_CHANGE_SETS_DIR}" -mindepth 1 -maxdepth 1 -type d -name "*upgrade-release*" -printf '%f\n' | sort -V | tail -n 1)

  local latest_upgrade_release_number="${latest_upgrade_release:0:1}"
  if [[ "${CHANGE_SET_NUMBER}" -lt "${latest_upgrade_release_number}" ]]; then
    print_error_and_exit "Invalid release change-set number. You can only restore to '${latest_upgrade_release}' change-set or later."
  fi
}

function create_or_restore_release_change_set() {
  local change_set_path

  if [[ "${CHANGE_SET_NUMBER}" == "next" ]]; then
    create_release_change_set
  else
    validate_release_number
    # There shouldn't be more than 1 release with same change set number so take the full output
    change_set_path=$(find "${LOCAL_USER_CHANGE_SETS_DIR}" -mindepth 1 -maxdepth 1 -type d -name "${CHANGE_SET_NUMBER}-release*")
    if [[ -z "${change_set_path}" || ! -d "${change_set_path}" ]]; then
      print_error_and_exit "Unknown release change-set: ${CHANGE_SET_NUMBER}"
    fi

    change_set_name=$(basename "${change_set_path}")
    wait_for_user_reply "Are you sure you want to restore the system to the ${change_set_name} change-set?"

    # Restore schemas
    cp "${change_set_path}configuration/schema.xml" "${LOCAL_USER_CONFIG_DIR}"
    cp "${change_set_path}configuration/security-schema.xml" "${LOCAL_USER_CONFIG_DIR}"
    if [[ "${change_set_name}" == *"upgrade-release"* ]]; then
      BACKUP_NAME="global-upgrade"
    else
      BACKUP_NAME="${change_set_name}"
    fi
    TASK="restore"
    YES_FLAG="true" # Given the user has already answered "yes", don't ask again on restore
    restore_from_backup
  fi
}

function run_task() {
  case "${TASK}" in
  "recreate")
    recreate_deployment
    ;;
  "create")
    create_deployment
    ;;
  "update")
    update_deployment
    ;;
  "clean")
    clean_deployment
    ;;
  "backup")
    create_backup
    ;;
  "restore")
    restore_from_backup
    ;;
  "upgrade")
    upgrade
    ;;
  "package")
    package
    ;;
  "release")
    create_or_restore_release_change_set
    ;;
  esac
}

function run_normal_deployment() {
  work_out_task_to_run
  run_task
}

function package() {
  initialize_deployment
  build_extensions
  build_plugins
  build_liberty_configured_image
}

function main() {
  run_top_level_checks
  print_deployment_information

  check_licenses_accepted_if_required "${ENVIRONMENT}" "${DEPLOYMENT_PATTERN}" "${DB_DIALECT}"
  validate_json_files

  # Cleaning up Docker resources
  clean_up_docker_resources
  create_docker_network "${DOMAIN_NAME}"

  local extra_args=()
  if [[ "${YES_FLAG}" == "true" ]]; then
    extra_args+=("-y")
  fi
  if [[ "${VERBOSE}" == "true" ]]; then
    extra_args+=("-v")
  fi
  "${ANALYZE_CONTAINERS_ROOT_DIR}/scripts/manage-environment" -t link "${extra_args[@]}"
  if [[ "${TASK}" != "clean" && "${TASK}" != "package" ]]; then
    if [[ ! -f "${PREVIOUS_STATE_FILE_PATH}" ]]; then
      # The deployment is not healthy, if generated config doesn't exist and at least 1 config container present
      if check_some_containers_exist; then
        wait_for_user_reply "The '${CONFIG_NAME}' is not in a healthy state. Continuing to run the 'deploy' task to return the system to a healthy state. This will permanently remove the data from the deployment."
      fi
    else
      # Restart exited Docker containers if generated config exists
      restart_docker_containers_for_config "${CONFIG_NAME}"
    fi
    restart_connectors_for_config
  fi

  if [[ -z "${TASK}" ]]; then
    run_normal_deployment
  else
    run_task
  fi
}

main "$@"
